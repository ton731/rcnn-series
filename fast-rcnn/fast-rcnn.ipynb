{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://github.com/zjZSTU/Fast-R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import torchmetrics\n",
    "import logging\n",
    "from torchvision.ops import roi_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'n_classes':21, 'max_proposals':2000, 'confidence_threshold': 0.95}\n",
    "train_config = {'epochs': 1, 'batch_size':2, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5, 'ckpt_dir': Path('results')}\n",
    "load_path = None\n",
    "# load_path = Path(\"results/2023_01_10__11_12_25\")\n",
    "voc_2012_classes = ['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset:\n",
    "    def __init__(self):\n",
    "        self.root = Path(\"../data\")\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "        self.train_dir = None\n",
    "        self.test_dir = None\n",
    "        self.train_data_link = None\n",
    "        self.test_data_link = None\n",
    "\n",
    "\n",
    "    def common_init(self):\n",
    "        # init for shared subclasses\n",
    "        self.label_type = ['none','aeroplane',\"Bicycle\",'bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'cow',\"Diningtable\",\"Dog\",\"Horse\",\"Motorbike\",'person', \"Pottedplant\",'sheep',\"Sofa\",\"Train\",\"TVmonitor\"]\n",
    "        self.convert_id = ['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]\n",
    "        self.convert_labels = {}\n",
    "        for i, x in enumerate(self.label_type):\n",
    "            self.convert_labels[x.lower()] = i\n",
    "\n",
    "        self.num_classes = len(self.label_type)     # 20 class + 1 background\n",
    "    \n",
    "\n",
    "    def download_dataset(self, validation_size=5000):\n",
    "        # download voc train dataset\n",
    "        if os.path.exists(self.root / \"voctrain.tar\"):\n",
    "            print(\"[*] Dataset already exists!\")\n",
    "        else:\n",
    "            print(\"[*] Downloading dataset...\")\n",
    "            print(self.train_data_link)\n",
    "            urllib.request.urlretrieve(self.train_data_link, self.root / \"voctrain.tar\")\n",
    "\n",
    "        if os.path.exists(self.root / \"VOCtrain\"):\n",
    "            print(\"[*] Dataset is already extracted!\")\n",
    "        else:\n",
    "            print(\"[*] Extracting dataset...\")\n",
    "            tar = tarfile.open(self.root / \"voctrain.tar\")\n",
    "            tar.extractall(self.root / \"VOCtrain\")\n",
    "            tar.close()\n",
    "\n",
    "        # download test dataset\n",
    "        if os.path.exists(self.root / \"VOCtest\"):\n",
    "            print(\"[*] Test dataset already exist!\")\n",
    "        else:\n",
    "            if self.test_data_link is None:\n",
    "                # move 5k images to validation set\n",
    "                print(\"[*] Moving validation data...\")\n",
    "                test_annotation_dir = self.test_dir / \"Annotations\"\n",
    "                test_img_dir = self.test_dir / \"JPEGImages\"\n",
    "                test_annotation_dir.mkdir(parents=True, exist_ok=True)\n",
    "                test_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                random.seed(731)\n",
    "                val_img_paths = random.sample(sorted(os.listdir(self.train_dir / \"JPEGImages\")), validation_size)\n",
    "\n",
    "                for path in val_img_paths:\n",
    "                    img_name = str(path).split(\"/\")[-1].split(\".\")[0]\n",
    "                    # move image\n",
    "                    os.rename(self.train_dir / \"JPEGImages\" / f\"{img_name}.jpg\", test_img_dir / f\"{img_name}.jpg\")\n",
    "                    # move annotation\n",
    "                    os.rename(self.train_dir / \"Annotations\" / f\"{img_name}.xml\", test_annotation_dir / f\"{img_name}.xml\")\n",
    "            else:\n",
    "                # load from val data\n",
    "                print(\"[*] Downloading validation dataset...\")\n",
    "                urllib.request.urlretrieve(self.test_data_link, \"voctset.tar\")\n",
    "\n",
    "                print(\"[*] Extracting validation dataset...\")\n",
    "                tar = tarfile.open(\"voctest.tar\", \"r:\")\n",
    "                tar.extractall(\"/content/VOCtest\")\n",
    "                tar.close()\n",
    "                # os.remove(\"/content/voctset.tar\")\n",
    "\n",
    "\n",
    "    def read_xml(self, xml_path):\n",
    "        object_list = []\n",
    "\n",
    "        tree = ET.parse(open(xml_path, 'r'))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        objects = root.findall(\"object\")\n",
    "        for _object in objects:\n",
    "            name = _object.find(\"name\").text\n",
    "            bndbox = _object.find(\"bndbox\")\n",
    "            xmin = int(bndbox.find(\"xmin\").text)\n",
    "            ymin = int(bndbox.find(\"ymin\").text)\n",
    "            xmax = int(bndbox.find(\"xmax\").text)\n",
    "            ymax = int(bndbox.find(\"ymax\").text)\n",
    "            class_name = _object.find(\"name\").text\n",
    "            object_list.append({'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax, 'class':self.convert_labels[class_name]})\n",
    "        \n",
    "        return object_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007(VOCDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_dir = self.root / 'VOCtrain/VOCdevkit/VOC2007'\n",
    "        self.test_dir = self.root / 'VOCtest/VOCdevkit/VOC2007'\n",
    "        self.train_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
    "        self.test_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar'\n",
    "        self.common_init()  #mandatory\n",
    "    \n",
    "class VOC2012(VOCDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_dir = self.root / 'VOCtrain/VOCdevkit/VOC2012'\n",
    "        self.test_dir = self.root / 'VOCtest/VOCdevkit/VOC2012'\n",
    "        # original site goes down frequently, so we use a link to the clone alternatively\n",
    "        # self.train_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar' \n",
    "        self.train_data_link = 'http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar'\n",
    "        self.test_data_link = None\n",
    "        self.common_init()  #mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Dataset already exists!\n",
      "[*] Dataset is already extracted!\n",
      "[*] Test dataset already exist!\n"
     ]
    }
   ],
   "source": [
    "voc_dataset = VOC2012()\n",
    "voc_dataset.download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data num: 12125\n",
      "valid data num: 5000\n"
     ]
    }
   ],
   "source": [
    "train_data_num = len(os.listdir(voc_dataset.train_dir / \"Annotations\"))\n",
    "valid_data_num = len(os.listdir(voc_dataset.test_dir / \"Annotations\"))\n",
    "print(\"train data num:\", train_data_num)\n",
    "print(\"valid data num:\", valid_data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img):\n",
    "    # return region proposals of selective searh over an image\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU (Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IoU(bb1, bb2):\n",
    "    # IoU = area_of_overlap / area_of_union\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "    \n",
    "    # return IoU as 0 if 2 boxes are not intersected\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    # calculate areas\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "    union_area = bb1_area + bb2_area - intersection_area\n",
    "\n",
    "    # iou\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMS (Non-max suppression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes, iou_threshold=0.5):\n",
    "    # bboxes: list of dicts {'bbox':(x1,x2,y1,y2), 'confidence':float, 'class':int}\n",
    "    confidence_list = np.array([bbox['confidence'] for bbox in bboxes])\n",
    "    confidence_order = (-confidence_list).argsort()   # apply minus to make the order descending\n",
    "    is_removed = [False for _ in range(len(bboxes))]\n",
    "    selected_bboxes = []\n",
    "    \n",
    "    for i in range(len(bboxes)):\n",
    "        bbox_idx = confidence_order[i]\n",
    "        if is_removed[bbox_idx]:\n",
    "            continue\n",
    "        \n",
    "        # add bbox to the main bbox object\n",
    "        selected_bboxes.append(bboxes[bbox_idx])\n",
    "        is_removed[bbox_idx] = True\n",
    "        \n",
    "        # remove overlapping bboxes\n",
    "        for order in range(i+1, len(bboxes)):\n",
    "            other_bbox_idx = confidence_order[order]\n",
    "            # check if the bbox not remove yet\n",
    "            if is_removed[other_bbox_idx] == False:\n",
    "                # check overlapping\n",
    "                iou = calculate_IoU(bboxes[bbox_idx]['bbox'], bboxes[other_bbox_idx]['bbox'])\n",
    "                if iou >= iou_threshold:\n",
    "                    is_removed[other_bbox_idx] = True\n",
    "    \n",
    "    return selected_bboxes\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and Dataloaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast R-CNN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    It's for domain-specific fine-tuning, inputs are the cropped image of the bounding\n",
    "    boxes, and outputs are the labels of the cropped images, such as background, class 1,\n",
    "    class 2, ... class N.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, cfg, IoU_threshold={'positive':0.5, 'negative':0.1},\n",
    "                 sample_ratio={'object':16, 'background':48}, data_path=Path(\"fast-rcnn_data/\")):\n",
    "        self.data_path = data_path\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.dataset = dataset\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if self.dataset_exists() == False:\n",
    "            self.generate_dataset(IoU_threshold)\n",
    "        else:\n",
    "            print(\"[*] Loading Fast-RCNN dataset from\", self.data_path)\n",
    "            with open(self.data_path / \"train_rois.pkl\", 'rb') as f:\n",
    "                self.train_rois = pickle.load(f)\n",
    "            with open(self.data_path / \"train_labels.pkl\", 'rb') as f:\n",
    "                self.train_labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_rois)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # get img\n",
    "        rois = self.train_rois[i]\n",
    "        img = Image.fromarray(cv2.cvtColor(rois['img'], cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # sample rois (use maximum 16 positive rois and 3x num for negative rois)\n",
    "        positive_sample_num = min(len(rois['positive']), self.sample_ratio['object'])\n",
    "        negative_sample_num = min(positive_sample_num * 3, len(rois['negative']))\n",
    "        # print(f\"positive num: {len(rois['positive'])}, sample num: {positive_sample_num}\")\n",
    "        # print(f\"negative num: {len(rois['negative'])}, sample num: {negative_sample_num}\")\n",
    "        positive_idxs = random.sample(list(range(len(rois['positive']))), positive_sample_num)\n",
    "        negative_idxs = random.sample(list(range(len(rois['negative']))), negative_sample_num)\n",
    "        positive_rois = [rois['positive'][idx] for idx in positive_idxs]\n",
    "        negative_rois = [rois['negative'][idx] for idx in negative_idxs]\n",
    "        sampled_rois = positive_rois + negative_rois\n",
    "        \n",
    "        # get labels\n",
    "        # classes\n",
    "        labels = self.train_labels[i]\n",
    "        positive_labels = [labels['positive'][idx][0] for idx in positive_idxs]\n",
    "        negative_labels = [labels['negative'][idx][0] for idx in negative_idxs]\n",
    "        sampled_labels = positive_labels + negative_labels\n",
    "        \n",
    "        # proposed roi xywh\n",
    "        positive_proposed_bboxes = [labels['positive'][idx][1] for idx in positive_idxs]\n",
    "        negative_proposed_bboxes = [labels['negative'][idx][1] for idx in negative_idxs]\n",
    "        sampled_proposed_bboxes = positive_proposed_bboxes + negative_proposed_bboxes\n",
    "        \n",
    "        # ground truth bbox xywh\n",
    "        positive_gt_bboxes = [labels['positive'][idx][2] for idx in positive_idxs]\n",
    "        negative_gt_bboxes = [labels['negative'][idx][2] for idx in negative_idxs]\n",
    "        sampled_gt_bboxes = positive_gt_bboxes + negative_gt_bboxes\n",
    "        \n",
    "        return {'image': self.transform(img), 'rois': sampled_rois, 'labels': sampled_labels,\n",
    "                'proposed_bboxes': sampled_proposed_bboxes, 'gt_bboxes': sampled_gt_bboxes}\n",
    "        \n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "        \"\"\"\n",
    "        Combine multiple data into a batch.\n",
    "\n",
    "        samples: List[Dict]\n",
    "        \n",
    "        return: Dict\n",
    "        \"\"\"\n",
    "        imgs = [sample['image'] for sample in samples]\n",
    "        rois = [sample['rois'] for sample in samples]\n",
    "        labels = [torch.tensor(sample['labels']) for sample in samples]\n",
    "        proposed_bboxes = [sample['proposed_bboxes'] for sample in samples]\n",
    "        gt_bboxes = [sample['gt_bboxes'] for sample in samples]        \n",
    "\n",
    "        batch = {'images': imgs, 'rois':rois, 'labels': labels, \n",
    "                 'proposed_bboxes': proposed_bboxes, 'gt_bboxes': gt_bboxes}\n",
    "        return batch\n",
    "\n",
    "        \n",
    "    def dataset_exists(self):\n",
    "        if os.path.exists(self.data_path / \"train_rois.pkl\") == False:\n",
    "            return False\n",
    "        if os.path.exists(self.data_path / \"train_labels.pkl\") == False:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def generate_dataset(self, IoU_threshold):\n",
    "        print(\"[*] Generating dataset for Fast-RCNN\")\n",
    "        image_dir = self.dataset.train_dir / \"JPEGImages\"\n",
    "        annot_dir = self.dataset.train_dir / \"Annotations\"\n",
    "        self.train_rois = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        # pbar = tqdm(sorted(os.listdir(image_dir)[:100]), position=0, leave=True)\n",
    "        pbar = tqdm(sorted(os.listdir(image_dir)), position=0, leave=True)\n",
    "        for img_name in pbar:\n",
    "            pbar.set_description(f\"Data size: {len(self.train_rois)}\")\n",
    "            \n",
    "            # load image and ground truth bounding boxes\n",
    "            img = cv2.imread(str(image_dir / img_name))\n",
    "            xml_path = annot_dir / img_name.replace(\".jpg\", \".xml\")\n",
    "            gt_bboxes = self.dataset.read_xml(xml_path)\n",
    "            \n",
    "            # generate bounding box proposals from selective search\n",
    "            rects = selective_search(img)[:2000]  # use only 2000 box\n",
    "            random.shuffle(rects)\n",
    "            \n",
    "            positive_rois = []\n",
    "            negative_rois = []\n",
    "            positive_labels = []\n",
    "            negative_labels = []\n",
    "            \n",
    "            # loop through all proposals\n",
    "            for (x, y, w, h) in rects:\n",
    "                # get the 4 points\n",
    "                x1, x2 = x, x + w\n",
    "                y1, y2 = y, y + h\n",
    "                proposed_bbox = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2}\n",
    "                \n",
    "                # check the proposal with every elements of the ground truth boxes\n",
    "                is_object = False   # define flag\n",
    "                between_negative_rois = False\n",
    "                for gt_bbox in gt_bboxes:\n",
    "                    iou = calculate_IoU(gt_bbox, proposed_bbox)\n",
    "                    # positive if iou >= 0.5\n",
    "                    if iou >= IoU_threshold['positive']:\n",
    "                        # add roi\n",
    "                        positive_rois.append(proposed_bbox)\n",
    "                        # add label, here x, y is the center of the rectangle\n",
    "                        proposed_bbox_xywh = ((x1+x2)/2, (y1+y2)/2, (x2-x1), (y2-y1))\n",
    "                        gt_bbox_xywh = ((gt_bbox['x1']+gt_bbox['x2'])/2, (gt_bbox['y1']+gt_bbox['y2'])/2, \n",
    "                                        (gt_bbox['x2']-gt_bbox['x1']), (gt_bbox['y2']-gt_bbox['y1']))\n",
    "                        positive_labels.append([gt_bbox['class'], proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                        is_object = True\n",
    "                        break\n",
    "                    else:\n",
    "                        if iou >= IoU_threshold['negative']:\n",
    "                            between_negative_rois = True\n",
    "                        \n",
    "                # if the proposal is not close to any ground truth box, and at least iou >= 0.1\n",
    "                if is_object == False and between_negative_rois:\n",
    "                    # add roi\n",
    "                    negative_rois.append(proposed_bbox)\n",
    "                    # add background label\n",
    "                    proposed_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                    gt_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                    negative_labels.append([0, proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                \n",
    "            # add to train data\n",
    "            self.train_rois.append({'img': img, 'positive': positive_rois, 'negative': negative_rois})\n",
    "            self.train_labels.append({'positive': positive_labels, 'negative': negative_labels})\n",
    "            # print(f\"positive : negative number = {len(positive_rois)} : {len(negative_rois)}\")\n",
    "\n",
    "        \n",
    "        print(\"[*] Dataset generated! Saving labels to \", self.data_path)\n",
    "        with open(self.data_path / \"train_rois.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_rois, f)\n",
    "        with open(self.data_path / \"train_labels.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_RCNN_DataLoader(voc_dataset, cfg, train_cfg, shuffle=True):\n",
    "    fast_rcnn_dataset = Fast_RCNN_Dataset(voc_dataset, cfg)\n",
    "    return torch.utils.data.DataLoader(fast_rcnn_dataset, batch_size=train_cfg['batch_size'], collate_fn=fast_rcnn_dataset.collate_fn, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Fast-RCNN dataset from fast-rcnn_data\n"
     ]
    }
   ],
   "source": [
    "fast_rcnn_loader = Fast_RCNN_DataLoader(voc_dataset, config, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes: torch.Size([2, 5])\n",
      "torch.Size([2, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import roi_pool\n",
    "img = torch.rand((1, 512, 38, 42))\n",
    "boxes = torch.tensor([\n",
    "        [0, 0, 0, 13.5, 15],\n",
    "        [0, 0, 0, 17, 16]\n",
    "    ]).to(torch.float)\n",
    "print(\"boxes:\", boxes.shape)\n",
    "result = roi_pool(img, boxes, (7, 7))\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_RoI(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        num_classes doesn not include background\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # VGG16 convolution layer setting, remove the last max-pooling 'M'\n",
    "        feature_list = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512]\n",
    "        \n",
    "        self.features = models.vgg.make_layers(feature_list)\n",
    "        self.roi_pool_output_size = (7, 7)\n",
    "        self.fully_connected_layer = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(4096, num_classes + 1)\n",
    "        self.bbox_reg = nn.Linear(4096, num_classes * 4)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        print(\"[*] Initializing VGG_RoI network...\")\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "       \n",
    "\n",
    "    def _rois_to_boxes(self, rois, img_shape, feature_map_shape):\n",
    "        boxes = []\n",
    "        # original image and feature map shape\n",
    "        H, W = img_shape[-2:]\n",
    "        H_map, W_map = feature_map_shape[-2:]\n",
    "        for roi in rois:\n",
    "            # shape for roi on original image: h, w\n",
    "            # shape for roi on feature map: h_map, w_map\n",
    "            x1, x2, y1, y2 = roi['x1'], roi['x2'], roi['y1'], roi['y2']\n",
    "            h, w = y2 - y1, x2 - x1\n",
    "            h_map = h * H_map / H\n",
    "            w_map = w * W_map / W\n",
    "            # adjust the top-left corner coordinate: (x1, y1) --> (x1_map, y1_map)\n",
    "            x1_map = x1 * W_map / W\n",
    "            y1_map = y1 * H_map / H\n",
    "            x2_map = x1_map + w_map\n",
    "            y2_map = y1_map + h_map\n",
    "            box = [0, x1_map, y1_map, x2_map, y2_map]   # 0 is for mini-batch id\n",
    "            boxes.append(box)\n",
    "        \n",
    "        boxes = torch.tensor(boxes).to(torch.float).to(device)\n",
    "        return boxes\n",
    "\n",
    "            \n",
    "    def forward(self, img, rois):\n",
    "        \"\"\"\n",
    "        img: [1, 3, H, W]\n",
    "        rois: [{x1, x2, y1, y2}, {x1, x2, y1, y2}, ...]\n",
    "        \"\"\"\n",
    "        # get feautre maps from CNN (VGG)\n",
    "        feature_map = self.features(img)\n",
    "        # get the box locations on the feature maps\n",
    "        boxes = self._rois_to_boxes(rois, img.shape, feature_map.shape)\n",
    "        # do the roi pooling to get the same-sized output map for each roi\n",
    "        x = roi_pool(feature_map, boxes, self.roi_pool_output_size)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # go through fc then classifier and regressor\n",
    "        x = self.fully_connected_layer(x)\n",
    "        object_classification = self.classifier(x)\n",
    "        bbox_regression = self.bbox_reg(x)\n",
    "        return object_classification, bbox_regression\n",
    "    \n",
    "\n",
    "    def refine_bbox(self, bbox, pred):\n",
    "        # bbox is list of [{x1, x2, y1, y2}, ...]\n",
    "        # pred is a sizr 4 array of predicted refinement of shape\n",
    "        x, y = (bbox['x1'] + bbox['x2']) / 2, (bbox['y1'] + bbox['y2']) / 2\n",
    "        w, h = bbox['x2'] - bbox['x1'], bbox['y2'] - bbox['y1']\n",
    "\n",
    "        new_x = x + w * pred[0]\n",
    "        new_y = y + h * pred[1]\n",
    "        new_w = w * torch.exp(pred[2])\n",
    "        new_h = h * torch.exp(pred[3])\n",
    "\n",
    "        return {'x1': new_x - new_w/2, 'x2': new_x + new_w/2, 'y1': new_y - new_h/2, 'y2': new_y + new_h/2}\n",
    "\n",
    "\n",
    "    def inference_single(self, img, rgb=False, batch_size=8, apply_nms=True, nms_threshold=0.2):\n",
    "        # img have to be loaded in BGR colorspace, or else rgb should be True\n",
    "        self.eval()\n",
    "        if rgb:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # perform selective search to find region proposals\n",
    "        rects = selective_search(img)\n",
    "        proposals = []\n",
    "        boxes = []\n",
    "        for (x, y, w, h) in rects[:self.max_proposals]:\n",
    "            roi = cv2.cvtColor(img[y:y+h, x:x+w, :], cv2.COLOR_RGB2BGR)\n",
    "            roi = preprocess(roi)\n",
    "            proposals.append(roi)\n",
    "            boxes.append({'x1':x, 'y1':y, 'x2':x+w, 'y2':y+h})\n",
    "        \n",
    "        # convert to dataloader for batching\n",
    "        proposals = torch.stack(proposals)\n",
    "        proposals = torch.Tensor(proposals)\n",
    "        proposals = torch.utils.data.TensorDataset(proposals)\n",
    "        proposals = torch.utils.data.DataLoader(proposals, batch_size=batch_size)\n",
    "\n",
    "        # predict probability of each box\n",
    "        bacth_iter = 0\n",
    "        useful_bboxes = []\n",
    "        for proposal_batch in tqdm(proposals):\n",
    "            patches = proposal_batch[0].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = self.convnet(patches)\n",
    "                features = self.flatten(features)\n",
    "                pred = self.classifier(features)\n",
    "                pred = self.softmax(pred)\n",
    "\n",
    "                if self.do_bbox_reg:\n",
    "                    bbox_refine = self.bbox_reg(features)\n",
    "            \n",
    "            # patches which are not classsified as background\n",
    "            useful_idx = torch.where(pred.argmax(1) > 0)    \n",
    "            for i in useful_idx[0]:\n",
    "                # loop through all useful patches\n",
    "                i = i.cpu().detach().numpy()\n",
    "                estimate = {}\n",
    "\n",
    "                class_prob = pred[i].cpu().detach().numpy()\n",
    "                estimate['class'] = class_prob.argmax(0)\n",
    "                estimate['confidence'] = class_prob.max(0)\n",
    "\n",
    "                original_bbox = boxes[bacth_iter * batch_size + i]\n",
    "                if self.do_bbox_reg == False:\n",
    "                    estimate['bbox'] = original_bbox\n",
    "                else:\n",
    "                    estimate['bbox'] = self.refine_bbox(original_bbox, bbox_refine[i])\n",
    "                \n",
    "                if estimate['confidence'] > self.confidence_threshold:\n",
    "                    useful_bboxes.append(estimate)\n",
    "\n",
    "            bacth_iter += 1\n",
    "\n",
    "        # apply non-max suppression to remove duplicate boxes\n",
    "        if apply_nms:\n",
    "            useful_bboxes = nms(useful_bboxes, nms_threshold)\n",
    "\n",
    "        # convert bboxes values to numpy\n",
    "        for i in range(len(useful_bboxes)):\n",
    "            useful_bboxes[i][\"bbox\"][\"x1\"] = int(useful_bboxes[i][\"bbox\"][\"x1\"].cpu().numpy())\n",
    "            useful_bboxes[i][\"bbox\"][\"y1\"] = int(useful_bboxes[i][\"bbox\"][\"y1\"].cpu().numpy())\n",
    "            useful_bboxes[i][\"bbox\"][\"x2\"] = int(useful_bboxes[i][\"bbox\"][\"x2\"].cpu().numpy())\n",
    "            useful_bboxes[i][\"bbox\"][\"y2\"] = int(useful_bboxes[i][\"bbox\"][\"y2\"].cpu().numpy())\n",
    "\n",
    "        \n",
    "        return useful_bboxes\n",
    "    \n",
    "\n",
    "    def inference(self, imgs, rgb=False, batch_size=8, apply_nms=True, nms_threshold=0.2):\n",
    "        # when given single image\n",
    "        if type(imgs) == np.ndarray and len(imgs.shape) == 3:\n",
    "            return self.inference_single(imgs, rgb, batch_size, apply_nms)\n",
    "\n",
    "        bboxes = []\n",
    "        for img in tqdm(imgs):\n",
    "            pred_bboxes = self.inference_single(img, rgb, batch_size, apply_nms)\n",
    "            bboxes.append(pred_bboxes)\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass MultiTaskLoss(nn.Module):\\n    def __init__(self, lam=1):\\n        super().__init__()\\n        self.lam = lam\\n        # L_cls: cross entropy loss for classification\\n        self.cls = nn.CrossEntropyLoss()\\n        # L_loc: smooth L1 loss for box location regression\\n        self.loc = SmoothL1Loss()\\n        \\n    def forward(self, preds, labels, bbox_est, bbox_ans):\\n        \"\"\"\\n        Multitask loss, where N is number of RoI\\n        :param preds: [N, C], C is the class number\\n        :param labels: [N], 0 is the background\\n        :param bbox_est: [N, 4], 4 is for x, y, w, h for boxes\\n        :param bbox_ans: [N, 4], 4 is for x, y, w, h for boxes\\n        \"\"\"\\n        cls_loss = self.cls(preds, labels)\\n        reg_loss = self.loc(bbox_est, bbox_ans)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmoothL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def smooth_l1(self, x):\n",
    "        condition = (torch.abs(x) < 1)\n",
    "        loss = (0.5 * torch.pow(x[condition], 2)).sum()\n",
    "        loss += (torch.abs(x[condition == False]) - 0.5).sum()\n",
    "        return loss\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        Calculate the regression loss for bounding box refinement.\n",
    "        Both shape of preds and targets are [N, 4], where N is the number of RoI.\n",
    "        \"\"\"\n",
    "        res = self.smooth_l1(preds - targets)\n",
    "        return torch.sum(res)\n",
    "\n",
    "\n",
    "'''\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, lam=1):\n",
    "        super().__init__()\n",
    "        self.lam = lam\n",
    "        # L_cls: cross entropy loss for classification\n",
    "        self.cls = nn.CrossEntropyLoss()\n",
    "        # L_loc: smooth L1 loss for box location regression\n",
    "        self.loc = SmoothL1Loss()\n",
    "        \n",
    "    def forward(self, preds, labels, bbox_est, bbox_ans):\n",
    "        \"\"\"\n",
    "        Multitask loss, where N is number of RoI\n",
    "        :param preds: [N, C], C is the class number\n",
    "        :param labels: [N], 0 is the background\n",
    "        :param bbox_est: [N, 4], 4 is for x, y, w, h for boxes\n",
    "        :param bbox_ans: [N, 4], 4 is for x, y, w, h for boxes\n",
    "        \"\"\"\n",
    "        cls_loss = self.cls(preds, labels)\n",
    "        reg_loss = self.loc(bbox_est, bbox_ans)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN_Trainer:\n",
    "\tdef __init__(self, model, fast_rcnn_loader, train_cfg):\n",
    "\t\tself.model = model\n",
    "\t\tself.fast_rcnn_loader = fast_rcnn_loader\n",
    "\t\tself.train_cfg = train_cfg\n",
    "\n",
    "\t\tself._init_env()\n",
    "\t\t\n",
    "\t\n",
    "\tdef _init_env(self):\n",
    "\t\t# checkpoint directory\n",
    "\t\tckpt_dir = self.train_cfg['ckpt_dir']\n",
    "\t\tckpt_dir = ckpt_dir / f'{datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")}'\n",
    "\t\tckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\t\tself.ckpt_dir = ckpt_dir\n",
    "\t \n",
    "\t\t# logger\n",
    "\t\tlogger = logging.getLogger(name='RCNN')\n",
    "\t\tlogger.setLevel(level=logging.INFO)\n",
    "\t\t# set formatter\n",
    "\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\t\t# console handler\n",
    "\t\tstream_handler = logging.StreamHandler()\n",
    "\t\tstream_handler.setFormatter(formatter)\n",
    "\t\tlogger.addHandler(stream_handler)\n",
    "\t\t# file handler\n",
    "\t\tfile_handler = logging.FileHandler(os.path.join(ckpt_dir, \"record.log\"))\n",
    "\t\tfile_handler.setFormatter(formatter)\n",
    "\t\tlogger.addHandler(file_handler)\n",
    "\t\t\n",
    "\t\tself.logger = logger\n",
    "\t\tself.logger.info(self.train_cfg)\n",
    "\t\n",
    "\t\n",
    "\tdef train(self):\n",
    "\t\t# define loss function\n",
    "\t\tcriterion_ce = nn.CrossEntropyLoss()\n",
    "\t\tcriterion_reg = SmoothL1Loss()\n",
    "\t\t\n",
    "\t\t# compute the gradients for the entire model\n",
    "\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=self.train_cfg['lr'], weight_decay=self.train_cfg['l2_reg'])\n",
    "\t\t\n",
    "\t\t# lr scheduler\n",
    "\t\tscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.train_cfg['lr_decay'])\n",
    "\t\t\n",
    "\t\t# initialize metrics\n",
    "\t\taccuracy_counter = torchmetrics.Accuracy(task=\"multiclass\", num_classes=21)\n",
    "\t\t\n",
    "\t\t# train loop\n",
    "\t\tbest_train_loss = np.inf\n",
    "\t\tfor epoch in range(self.train_cfg['epochs']):\n",
    "\t\t\tself.model.train()\n",
    "\t\t\tprint(f\"[*] Training Fast-RCNN epoch {epoch+1}/{self.train_cfg['epochs']}\")\n",
    "\t\t\tpbar = tqdm(self.fast_rcnn_loader)\n",
    "\t\t\ttrain_loss = 0\n",
    "\t\t\ttrain_acc = 0\n",
    "\t\t\ttrain_iter = 0\n",
    "\t\t\tfor step, batch in enumerate(pbar):\n",
    "\t\t\t\t# loop through the images in the batch\n",
    "\t\t\t\tloss = 0\n",
    "\t\t\t\tacc = 0\n",
    "\t\t\t\tN = len(batch['images'])\n",
    "\t\t\t\tfor i in range(N):\n",
    "\t\t\t\t\timg = batch['images'][i].unsqueeze(0).to(device)\n",
    "\t\t\t\t\trois = batch['rois'][i]\n",
    "\t\t\t\t\tproposed_bboxes = batch['proposed_bboxes'][i]\n",
    "\t\t\t\t\tgt_bboxes = batch['gt_bboxes'][i]\n",
    "\t\t\t\t\tlabels = batch['labels'][i].to(device)\n",
    "\n",
    "\t\t\t\t\tprint(\"roi num:\", len(rois))\n",
    "\t\t\t\t\tprint(\"proposed box num:\", len(proposed_bboxes))\n",
    "\n",
    "\t\t\t\t\t# foward\n",
    "\t\t\t\t\tpreds, bbox_est = self.model(img, rois)\n",
    "\n",
    "\t\t\t\t\tprint(\"bbox_est:\", bbox_est.shape)\n",
    "\n",
    "\t\t\t\t\t# classification loss\n",
    "\t\t\t\t\tloss += criterion_ce(preds, labels)\n",
    "\n",
    "\t\t\t\t\t# bounding box regression process\n",
    "\t\t\t\t\tfor bbox_idx in range(len(rois)):\n",
    "\t\t\t\t\t\t# count only bboxes that are not backgrond\n",
    "\t\t\t\t\t\tif labels[bbox_idx] > 1:\n",
    "\t\t\t\t\t\t\tp_x, p_y, p_w, p_h = proposed_bboxes[i][0], proposed_bboxes[i][1], proposed_bboxes[i][2], proposed_bboxes[i][3]\n",
    "\t\t\t\t\t\t\tg_x, g_y, g_w, g_h = gt_bboxes[i][0], gt_bboxes[i][1], gt_bboxes[i][2], gt_bboxes[i][3]\n",
    "\n",
    "\t\t\t\t\t\t\t# bbox_ans = torch.stack([(g_x-p_x)/p_w, (g_y-p_y)/p_h, torch.log(g_w)/p_w, torch.log(g_h)/p_h], axis=1)\n",
    "\t\t\t\t\t\t\tbbox_ans = torch.tensor([(g_x-p_x)/p_w, (g_y-p_y)/p_h, torch.log(torch.tensor(g_w))/p_w, torch.log(torch.tensor(g_h))/p_h])\n",
    "\t\t\t\t\t\t\tbbox_ans = bbox_ans.float().to(device)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tbbox_pred_class = preds[bbox_idx].argmax()\n",
    "\t\t\t\t\t\t\tbbox_pred = bbox_est[bbox_idx, bbox_pred_class*4:bbox_pred_class*4+4]\n",
    "\n",
    "\t\t\t\t\t\t\t# calculate regression loss\n",
    "\t\t\t\t\t\t\tprint(\"get loss, bbox_pred:\", bbox_pred)\n",
    "\t\t\t\t\t\t\tprint(\"get loss, bbox_ans :\", bbox_ans)\n",
    "\t\t\t\t\t\t\tloss += criterion_reg(bbox_pred, bbox_ans)\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t# calculate accuracy\n",
    "\t\t\t\t\tacc += accuracy_counter(preds.cpu(), labels.cpu()).numpy()\n",
    "\n",
    "\t\t\t\t# backpropagation\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\ttrain_loss += loss.cpu().detach().numpy()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# logging\n",
    "\t\t\t\ttrain_acc += acc\n",
    "\t\t\t\ttrain_iter += N\n",
    "\t\t\t\tpbar.set_description(f\"loss: {loss.cpu().detach().numpy():3f},  accuracy: {acc/N:.3f}\")\n",
    "\n",
    "\t\t\tself.logger.info(f\"Epoch: {epoch+1}, loss: {train_loss:3f},  accuracy: {train_acc/train_iter:.3f}\")\n",
    "\t\t\t\n",
    "\t\t\t# save model\n",
    "\t\t\tif train_loss < best_train_loss:\n",
    "\t\t\t\tbest_train_loss = train_loss\n",
    "\t\t\t\ttorch.save(self.model.state_dict(), self.ckpt_dir / \"model.pt\")\n",
    "\t\t\t\n",
    "\t\t\t# update lr\n",
    "\t\t\tscheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing VGG_RoI network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 10:40:34,006 - RCNN - INFO - {'epochs': 1, 'batch_size': 2, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n",
      "2023-01-18 10:40:34,006 - RCNN - INFO - {'epochs': 1, 'batch_size': 2, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n",
      "2023-01-18 10:40:34,006 - RCNN - INFO - {'epochs': 1, 'batch_size': 2, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n",
      "2023-01-18 10:40:34,006 - RCNN - INFO - {'epochs': 1, 'batch_size': 2, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n",
      "2023-01-18 10:40:34,006 - RCNN - INFO - {'epochs': 1, 'batch_size': 2, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n",
      "2023-01-18 10:40:34,006 - RCNN - INFO - {'epochs': 1, 'batch_size': 2, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n"
     ]
    }
   ],
   "source": [
    "model = VGG_RoI(config['n_classes']).to(device)\n",
    "if load_path:\n",
    "    model.load_state_dict(torch.load(load_path / \"model.pt\"))\n",
    "trainer = Fast_RCNN_Trainer(model, fast_rcnn_loader, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Training Fast-RCNN epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roi num: 64\n",
      "proposed box num: 64\n",
      "bbox_est: torch.Size([64, 84])\n",
      "get loss, bbox_pred: tensor([-0.0028, -0.0125, -0.0230, -0.0777], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0698, -0.0918,  0.0178, -0.0870], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0026,  0.0365, -0.0360, -0.0597], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0019, -0.0291,  0.0868, -0.0276], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0261, -0.0945, -0.1194, -0.0737], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0499, -0.0762, -0.0167,  0.0547], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0376, -0.0641, -0.0341, -0.0992], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.1039, -0.0638, -0.1248,  0.0068], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0416, -0.0639,  0.0133, -0.1219], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0948, -0.0114,  0.0248, -0.0307], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0968,  0.0094, -0.0220, -0.1054], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0919, -0.0587, -0.1068,  0.1249], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0691,  0.0738,  0.0046, -0.0649], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0343, -0.0569, -0.0028, -0.0380], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([ 0.0419, -0.0306,  0.0375, -0.0967], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n",
      "get loss, bbox_pred: tensor([-0.0311, -0.1196, -0.1115, -0.0328], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "get loss, bbox_ans : tensor([0.0317, 0.2195, 0.0223, 0.0226], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If `preds` have one dimension more than `target`, `preds.shape[1]` should be equal to number of classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[34], line 102\u001b[0m, in \u001b[0;36mFast_RCNN_Trainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \t\t\tloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion_reg(bbox_pred, bbox_ans)\n\u001b[0;32m    101\u001b[0m \t\u001b[39m# calculate accuracy\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m \tacc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m accuracy_counter(preds\u001b[39m.\u001b[39;49mcpu(), labels\u001b[39m.\u001b[39;49mcpu())\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    104\u001b[0m \u001b[39m# backpropagation\u001b[39;00m\n\u001b[0;32m    105\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torchmetrics\\metric.py:234\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_full_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_reduce_state_update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torchmetrics\\metric.py:300\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    301\u001b[0m batch_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute()\n\u001b[0;32m    303\u001b[0m \u001b[39m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torchmetrics\\metric.py:388\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[0;32m    387\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 388\u001b[0m         update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    389\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    390\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torchmetrics\\classification\\stat_scores.py:316\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update state with predictions and targets.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[0;32m    311\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39m    preds: Tensor with predictions\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m    target: Tensor with true labels\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_args:\n\u001b[1;32m--> 316\u001b[0m     _multiclass_stat_scores_tensor_validation(\n\u001b[0;32m    317\u001b[0m         preds, target, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultidim_average, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index\n\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    319\u001b[0m preds, target \u001b[39m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k)\n\u001b[0;32m    320\u001b[0m tp, fp, tn, fn \u001b[39m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[0;32m    321\u001b[0m     preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultidim_average, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index\n\u001b[0;32m    322\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torchmetrics\\functional\\classification\\stat_scores.py:274\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[1;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf `preds` have one dimension more than `target`, `preds` should be a float tensor.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m num_classes:\n\u001b[1;32m--> 274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf `preds` have one dimension more than `target`, `preds.shape[1]` should be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m equal to number of classes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m     )\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:] \u001b[39m!=\u001b[39m target\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf `preds` have one dimension more than `target`, the shape of `preds` should be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (N, C, ...), and the shape of `target` should be (N, ...).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: If `preds` have one dimension more than `target`, `preds.shape[1]` should be equal to number of classes."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize(voc_dataset, model, img_path, i, save_to_disk=False):\n",
    "    img = cv2.imread(img_path)\n",
    "    pred_bboxes = model.inference(img, apply_nms=True, nms_threshold=0.2)\n",
    "    pred_bboxes=nms(pred_bboxes, 0.2)\n",
    "    # plot predicted\n",
    "    # print(\"Predicted:\")\n",
    "    predcited_name = f\"{i}_pred.png\"\n",
    "    plot_bboxes(img, pred_bboxes, predcited_name, save_to_disk)\n",
    "    # plot ground truth\n",
    "    xml_path = img_path.replace(\"JPEGImages\", \"Annotations\").replace(\".jpg\", \".xml\")\n",
    "    gt_bboxes = read_gt_xml(xml_path)\n",
    "    # print(\"Ground truth\")\n",
    "    gt_name = f\"{i}_truth.png\"\n",
    "    plot_bboxes(img, gt_bboxes, gt_name, save_to_disk)\n",
    "\n",
    "\n",
    "def read_gt_xml(xml_path):\n",
    "    tree = ET.parse(open(xml_path, 'r'))\n",
    "\n",
    "    root=tree.getroot()\n",
    "\n",
    "    obj_list = []\n",
    "    objects = root.findall(\"object\")\n",
    "    for _object in objects:\n",
    "        name = _object.find(\"name\").text\n",
    "        bndbox = _object.find(\"bndbox\")\n",
    "        xmin = int(bndbox.find(\"xmin\").text)\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "        class_name = _object.find('name').text\n",
    "\n",
    "        obj_list.append({'class': voc_dataset.convert_labels[class_name], 'confidence': 1.0, \n",
    "                         'bbox': {'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax}})\n",
    "    return obj_list\n",
    "\n",
    "\n",
    "\n",
    "def plot_bboxes(img, bboxes, save_name, save_to_disk, color=(255, 69, 0)):\n",
    "    plot_cfg = {'bbox_color':color, 'bbox_thickness':2, \n",
    "                'font':cv2.FONT_HERSHEY_SIMPLEX, 'fontScale':0.5, 'fontColor':color, 'lineThickness':1}\n",
    "    img_bbox = img.copy()\n",
    "    img_bbox = cv2.cvtColor(img_bbox, cv2.COLOR_BGR2RGB)\n",
    "    for box in bboxes:\n",
    "        bbox = box['bbox']\n",
    "        cv2.rectangle(img_bbox, (bbox['x1'], bbox['y1']), (bbox['x2'], bbox['y2']), plot_cfg['bbox_color'], plot_cfg['bbox_thickness'])\n",
    "        cv2.putText(img_bbox, f\"{voc_dataset.label_type[box['class']]}, {str(box['confidence'])[:5]}\",  (bbox['x1'], bbox['y1'] - 5), plot_cfg['font'], \n",
    "                    plot_cfg['fontScale'], plot_cfg['fontColor'], plot_cfg['lineThickness'])\n",
    "    plt.imshow(img_bbox)\n",
    "    if save_to_disk:\n",
    "        plt.savefig(trainer.save_result_dir / save_name)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save\n",
    "trainer.save_result_dir = trainer.ckpt_dir / \"predictions\"\n",
    "trainer.save_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "plot_num = 200\n",
    "for i in range(plot_num):\n",
    "    img_name = sorted(os.listdir(voc_dataset.test_dir / \"JPEGImages\"))[i]\n",
    "    img_path = str(voc_dataset.test_dir / \"JPEGImages\" / img_name)\n",
    "    print(\"plotting:\", img_path)\n",
    "    visualize(voc_dataset, model, img_path, i=i, save_to_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efa59ba0514c88bea28add7efbea712f86beb100fe6589447027613f52b616e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
