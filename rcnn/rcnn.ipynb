{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/codex/implementing-r-cnn-object-detection-on-voc2012-with-pytorch-b05d3c623afe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'image_size':224, 'n_classes':21, 'bbox_reg': True, 'network': 'efficientnet-b0', 'max_proposals':2000, 'pad': 16}\n",
    "train_config = {'log_wandb':True, 'logging': ['plot'],\n",
    "              'epochs': 3, 'batch_size':8, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5}\n",
    "train_config_classifer = {'log_wandb':True, 'logging': ['plot'],\n",
    "              'epochs': 1, 'batch_size':8, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5}\n",
    "\n",
    "voc_2012_classes = ['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset:\n",
    "    def __init__(self):\n",
    "        self.root = Path(\"data\")\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "        self.train_dir = None\n",
    "        self.test_dir = None\n",
    "        self.train_data_link = None\n",
    "        self.test_data_link = None\n",
    "\n",
    "\n",
    "    def common_init(self):\n",
    "        # init for shared subclasses\n",
    "        self.label_type = ['none','aeroplane',\"Bicycle\",'bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'cow',\"Diningtable\",\"Dog\",\"Horse\",\"Motorbike\",'person', \"Pottedplant\",'sheep',\"Sofa\",\"Train\",\"TVmonitor\"]\n",
    "        self.convert_id = ['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]\n",
    "        self.convert_labels = {}\n",
    "        for i, x in enumerate(self.label_type):\n",
    "            self.convert_labels[x.lower()] = i\n",
    "\n",
    "        self.num_classes = len(self.label_type)     # 20 class + 1 background\n",
    "    \n",
    "\n",
    "    def download_dataset(self, validation_size=5000):\n",
    "        # download voc train dataset\n",
    "        if os.path.exists(self.root / \"voctrain.tar\"):\n",
    "            print(\"[*] Dataset already exists!\")\n",
    "        else:\n",
    "            print(\"[*] Downloading dataset...\")\n",
    "            print(self.train_data_link)\n",
    "            urllib.request.urlretrieve(self.train_data_link, self.root / \"voctrain.tar\")\n",
    "\n",
    "        if os.path.exists(self.root / \"VOCtrain\"):\n",
    "            print(\"[*] Dataset is already extracted!\")\n",
    "        else:\n",
    "            print(\"[*] Extracting dataset...\")\n",
    "            tar = tarfile.open(self.root / \"voctrain.tar\")\n",
    "            tar.extractall(self.root / \"VOCtrain\")\n",
    "            tar.close()\n",
    "\n",
    "        # download test dataset\n",
    "        if os.path.exists(self.root / \"VOCtest\"):\n",
    "            print(\"[*] Test dataset already exist!\")\n",
    "        else:\n",
    "            if self.test_data_link is None:\n",
    "                # move 5k images to validation set\n",
    "                print(\"[*] Moving validation data...\")\n",
    "                test_annotation_dir = self.test_dir / \"Annotations\"\n",
    "                test_img_dir = self.test_dir / \"JPEGImages\"\n",
    "                test_annotation_dir.mkdir(parents=True, exist_ok=True)\n",
    "                test_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                random.seed(731)\n",
    "                val_img_paths = random.sample(sorted(os.listdir(self.train_dir / \"JPEGImages\")), validation_size)\n",
    "\n",
    "                for path in val_img_paths:\n",
    "                    img_name = str(path).split(\"/\")[-1].split(\".\")[0]\n",
    "                    # move image\n",
    "                    os.rename(self.train_dir / \"JPEGImages\" / f\"{img_name}.jpg\", test_img_dir / f\"{img_name}.jpg\")\n",
    "                    # move annotation\n",
    "                    os.rename(self.train_dir / \"Annotations\" / f\"{img_name}.xml\", test_annotation_dir / f\"{img_name}.xml\")\n",
    "            else:\n",
    "                # load from val data\n",
    "                print(\"[*] Downloading validation dataset...\")\n",
    "                urllib.request.urlretrieve(self.test_data_link, \"voctset.tar\")\n",
    "\n",
    "                print(\"[*] Extracting validation dataset...\")\n",
    "                tar = tarfile.open(\"voctest.tar\", \"r:\")\n",
    "                tar.extractall(\"/content/VOCtest\")\n",
    "                tar.close()\n",
    "                # os.remove(\"/content/voctset.tar\")\n",
    "\n",
    "\n",
    "    def read_xml(self, xml_path):\n",
    "        object_list = []\n",
    "\n",
    "        tree = ET.parse(open(xml_path, 'r'))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        objects = root.findall(\"object\")\n",
    "        for _object in objects:\n",
    "            name = _object.find(\"name\").text\n",
    "            bndbox = _object.find(\"bndbox\")\n",
    "            xmin = int(bndbox.find(\"xmin\").text)\n",
    "            ymin = int(bndbox.find(\"ymin\").text)\n",
    "            xmax = int(bndbox.find(\"xmax\").text)\n",
    "            ymax = int(bndbox.find(\"ymax\").text)\n",
    "            class_name = _object.find(\"name\").text\n",
    "            object_list.append({'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax, 'class':self.convert_labels[class_name]})\n",
    "        \n",
    "        return object_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007(VOCDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_dir = self.root / 'VOCtrain/VOCdevkit/VOC2007'\n",
    "        self.test_dir = self.root / 'VOCtest/VOCdevkit/VOC2007'\n",
    "        self.train_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
    "        self.test_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar'\n",
    "        self.common_init()  #mandatory\n",
    "    \n",
    "class VOC2012(VOCDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_dir = self.root / 'VOCtrain/VOCdevkit/VOC2012'\n",
    "        self.test_dir = self.root / 'VOCtest/VOCdevkit/VOC2012'\n",
    "        # original site goes down frequently, so we use a link to the clone alternatively\n",
    "        # self.train_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar' \n",
    "        self.train_data_link = 'http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar'\n",
    "        self.test_data_link = None\n",
    "        self.common_init()  #mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dataset = VOC2012()\n",
    "voc_dataset.download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = len(os.listdir(voc_dataset.train_dir / \"Annotations\"))\n",
    "valid_data_num = len(os.listdir(voc_dataset.test_dir / \"Annotations\"))\n",
    "print(\"train data num:\", train_data_num)\n",
    "print(\"valid data num:\", valid_data_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img_bgr):\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_img_from_path(img_path):\n",
    "    print(\"plotting:\", img_path)\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    print(\"img shape:\", img_rgb.shape)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_bounding_box_from_path(img_path):\n",
    "    # img\n",
    "    print(\"plotting:\", img_path)\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    xml_path = img_path.replace(\"JPEGImages\", \"Annotations\").replace(\".jpg\", \".xml\")\n",
    "    tree = ET.parse(open(xml_path, 'r'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # print image shape\n",
    "    w, h = root.find(\"size\").find(\"width\").text, root.find(\"size\").find(\"height\").text\n",
    "    print(\"width, height:\", w, h)\n",
    "\n",
    "    # bounding box settings\n",
    "    box_img = img_bgr.copy()\n",
    "    bbox_color = (0, 69, 255)    # bgr\n",
    "    bbox_thickness = 2\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_color = bbox_color\n",
    "    line_type = 1\n",
    "\n",
    "    # plot box\n",
    "    objects = root.findall(\"object\")\n",
    "    for _object in objects:\n",
    "        name = _object.find(\"name\").text\n",
    "        bndbox = _object.find(\"bndbox\")\n",
    "        xmin = int(bndbox.find(\"xmin\").text)\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "        class_name = _object.find('name').text\n",
    "\n",
    "        cv2.rectangle(box_img, (xmin, ymin), (xmax, ymax), bbox_color, bbox_thickness)\n",
    "        cv2.putText(box_img, class_name, (xmin, ymin-5), font, font_scale, font_color, line_type)\n",
    "\n",
    "    box_img_rgb = cv2.cvtColor(box_img, cv2.COLOR_BGR2RGB)\n",
    "    result = np.hstack((img_rgb, box_img_rgb))\n",
    "    # plt.imshow(result)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "    plt.imshow(box_img_rgb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = random.choice(os.listdir(voc_dataset.train_dir / \"JPEGImages\"))\n",
    "img_path = str(voc_dataset.train_dir / \"JPEGImages\" / img_name)\n",
    "plot_bounding_box_from_path(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img):\n",
    "    # return region proposals of selective searh over an image\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    return rects\n",
    "\n",
    "\n",
    "def plot_results(img, bboxes, color=(0, 69, 255)):\n",
    "    plot_cfg = {'bbox_color':color, 'bbox_thickness':2, \n",
    "                'font':cv2.FONT_HERSHEY_SIMPLEX, 'font_scale':0.5, 'font_color':color, 'line_thickness':1}\n",
    "    img_bb = img.copy()\n",
    "    for box in bboxes:\n",
    "        bbox = box['bbox']\n",
    "        cv2.rectangle(img_bb, (bbox['x1'], bbox['y1']), (bbox['x2'], bbox['y2']), plot_cfg['bbox_color'], plot_cfg['bbox_thickness'])\n",
    "        cv2.putText(img_bb, f\"{voc_dataset.label_type[box['class']]}, {str(box['conf'])[:5]}\",  (bbox['x1'], bbox['y1'] - 5), plot_cfg['font'], \n",
    "                    plot_cfg['font_scale'], plot_cfg['font_color'], plot_cfg['line_thickness'])\n",
    "    return img_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = str(voc_dataset.train_dir / \"JPEGImages\" / img_name)\n",
    "img = cv2.imread(img_path)\n",
    "rects = selective_search(img)\n",
    "for i, rect in enumerate(rects):\n",
    "    if i > 2000:\n",
    "        break\n",
    "    x, y, w, h = rect\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (100, 255, 100), 1)\n",
    "plot_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU (Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IoU(bb1, bb2):\n",
    "    # IoU = area_of_overlap / area_of_union\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "    \n",
    "    # return IoU as 0 if 2 boxes are not intersected\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    # calculate areas\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "    union_area = bb1_area + bb2_area - intersection_area\n",
    "\n",
    "    # iou\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb1 = {'x1':100, 'x2':200, 'y1':50, 'y2':150}\n",
    "bb2 = {'x1':150, 'x2':300, 'y1':120, 'y2':200}\n",
    "print(\"my iou:\", calculate_IoU(bb1, bb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mAP (Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(pred, truth, iou_threshold=0.5, num_classes=21, per_class=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMS (Non-max suppression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes, iou_threshold=0.5):\n",
    "    # bboxes: list of dicts {'bbox':(x1,x2,y1,y2), 'confidence':float, 'class':int}\n",
    "    confidence_list = np.array([bbox['confidence'] for bbox on bboxes])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Domain-specific fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    It's for domain-specific fine-tuning, inputs are the cropped image of the bounding\n",
    "    boxes, and outputs are the labels of the cropped images, such as background, class 1,\n",
    "    class 2, ... class N.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, cfg, IoU_threshold={'positive':0.5, 'partial':0.3},\n",
    "                 sample_ratio={'object':32, 'background':96}, data_path=Path(\"data/domain-specific\")):\n",
    "        self.data_path = data_path\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if self.dataset_exists() == False:\n",
    "            self.generate_dataset(sample_ratio, IoU_threshold)\n",
    "        else:\n",
    "            print(\"[*] Loading domain-specific dataset from\", self.data_path)\n",
    "            with open(self.data_path / \"train_images.pkl\", 'rb') as f:\n",
    "                self.train_images = pickle.load(f)\n",
    "            with open(self.data_path / \"train_labels.pkl\", 'rb') as f:\n",
    "                self.train_labels = pickle.load(f)\n",
    "                \n",
    "            # check if both files are complete and flawless\n",
    "            if not len(self.train_images) == len(self.train_labels):\n",
    "                raise ValueError(\"The loaded dataset is invalid or in different size.\")\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if torch.is_tensor(i):\n",
    "            i = i.tolist()\n",
    "        img = Image.fromarray(cv2.cvtColor(self.train_images[i], cv2.COLOR_BGR2RGB))\n",
    "        return {'image': self.transform(img), 'label': self.train_labels[i][0],\n",
    "                'proposed_bbox': self.train_labels[i][1], 'gt_bbox': self.train_labels[i][2]}\n",
    "        \n",
    "        \n",
    "    def dataset_exists(self):\n",
    "        if os.path.exists(self.data_path / \"train_images.pkl\") == False:\n",
    "            return False\n",
    "        if os.path.exists(self.data_path / \"train_labels.pkl\") == False:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def generate_dataset(self, sample_ratio, IoU_threshold, padding=16):\n",
    "        print(\"[*] Generating dataset for R-CNN\")\n",
    "        image_dir = self.dataset.train_dir / \"JPEGImages\"\n",
    "        annot_dir = self.dataset.train_dir / \"Annotations\"\n",
    "        object_counter = 0\n",
    "        background_counter = 0\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        pbar = tqdm(sorted(os.listdir(image_dir))[:2000], position=0, leave=True)\n",
    "        for img_name in pbar:\n",
    "            pbar.set_description(f\"Data size: {len(self.train_labels)}\")\n",
    "            \n",
    "            # load image and ground truth bounding boxes\n",
    "            img = cv2.imread(str(image_dir / img_name))\n",
    "            xml_path = annot_dir / img_name.replace(\".jpg\", \".xml\")\n",
    "            gt_bboxes = self.dataset.read_xml(xml_path)\n",
    "            \n",
    "            # generate bounding box proposals from selective search\n",
    "            rects = selective_search(img)[:2000]  # use only 2000 box\n",
    "            random.shuffle(rects)\n",
    "            \n",
    "            # loop through all proposals\n",
    "            for (x, y, w, h) in rects:\n",
    "                # apply padding\n",
    "                x1, x2 = np.clip([x - padding, x + w + padding], 0, img.shape[1])\n",
    "                y1, y2 = np.clip([y - padding, y + h + padding], 0, img.shape[0])\n",
    "                proposed_bbox = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2}\n",
    "                \n",
    "                # check the proposal with every elements of the ground truth boxes\n",
    "                is_object = False   # define flag\n",
    "                for gt_bbox in gt_bboxes:\n",
    "                    iou = calculate_IoU(gt_bbox, proposed_bbox)\n",
    "                    if iou >= IoU_threshold['positive']:    # iou >= 0.5\n",
    "                        object_counter += 1\n",
    "                        # add image\n",
    "                        cropped = img[y1 : y2, x1 : x2, :]\n",
    "                        self.train_images.append(cropped)\n",
    "                        # add label, here x, y is the center of the rectangle\n",
    "                        proposed_bbox_xywh = ((x1+x2)/2, (y1+y2)/2, (x2-x1), (y2-y1))\n",
    "                        gt_bbox_xywh = ((gt_bbox['x1']+gt_bbox['x2'])/2, (gt_bbox['y1']+gt_bbox['y2'])/2, \n",
    "                                        (gt_bbox['x2']-gt_bbox['x1']), (gt_bbox['y2']-gt_bbox['y1']))\n",
    "                        self.train_labels.append([gt_bbox['class'], proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                        \n",
    "                        is_object = True\n",
    "                        break\n",
    "                \n",
    "                # if the proposal is not close to any ground truth box\n",
    "                if background_counter < sample_ratio['background'] and is_object == False:\n",
    "                    background_counter += 1\n",
    "                    # add background image\n",
    "                    cropped = img[y1 : y2, x1 : x2, :]\n",
    "                    self.train_images.append(cropped)\n",
    "                    # add background label\n",
    "                    proposed_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                    gt_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                    self.train_labels.append([0, proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                \n",
    "                # control the ratio between object and backgruond\n",
    "                if object_counter >= sample_ratio['object'] and background_counter == sample_ratio['background']:\n",
    "                    object_counter -= sample_ratio['object']\n",
    "                    background_counter = 0\n",
    "        \n",
    "        print(\"[*] Dataset generated! Saving labels to \", self.data_path)\n",
    "        with open(self.data_path / \"train_images.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_images, f)\n",
    "        with open(self.data_path / \"train_labels.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RCNN_DataLoader(voc_dataset, cfg, train_cfg, shuffle=True):\n",
    "    rcnn_dataset = RCNN_Dataset(voc_dataset, cfg)\n",
    "    return torch.utils.data.DataLoader(rcnn_dataset, batch_size=train_cfg['batch_size'], shuffle=shuffle, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_loader = RCNN_DataLoader(voc_dataset, config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "for x in rcnn_loader:\n",
    "    for i in range(5):\n",
    "        plt.imshow(inverse_transform(x['image'][i]))\n",
    "        plt.title(voc_2012_classes[x['label'][i]])\n",
    "        plt.show()\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Object category classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Classification_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    It's for the final object category classification model. The positive samples\n",
    "    are from the ground truth bboxes, and the negative samples are from the bbox\n",
    "    proposed with IoU smaller than 0.3.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, cfg, IoU_threshold={'positive':0.5, 'partial':0.3},\n",
    "                 sample_ratio={'object':32, 'background':96}, data_path=Path(\"data/object-classification\")):\n",
    "        self.data_path = data_path\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if self.dataset_exists() == False:\n",
    "            self.generate_dataset(sample_ratio, IoU_threshold)\n",
    "        else:\n",
    "            print(\"[*] Loading object-classification dataset from\", self.data_path)\n",
    "            with open(self.data_path / \"train_images_classification.pkl\", 'rb') as f:\n",
    "                self.train_images = pickle.load(f)\n",
    "            with open(self.data_path / \"train_labels_classification.pkl\", 'rb') as f:\n",
    "                self.train_labels = pickle.load(f)\n",
    "                \n",
    "            # check if both files are complete and flawless\n",
    "            if not len(self.train_images) == len(self.train_labels):\n",
    "                raise ValueError(\"The loaded dataset is invalid or in different size.\")\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if torch.is_tensor(i):\n",
    "            i = i.tolist()\n",
    "        img = Image.fromarray(cv2.cvtColor(self.train_images[i], cv2.COLOR_BGR2RGB))\n",
    "        return {'image': self.transform(img), 'label': self.train_labels[i][0],\n",
    "                'proposed_bbox': self.train_labels[i][1], 'gt_bbox': self.train_labels[i][2]}\n",
    "        \n",
    "        \n",
    "    def dataset_exists(self):\n",
    "        if os.path.exists(self.data_path / \"train_images_classification.pkl\") == False:\n",
    "            return False\n",
    "        if os.path.exists(self.data_path / \"train_labels_classification.pkl\") == False:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def generate_dataset(self, sample_ratio, IoU_threshold, padding=16):\n",
    "        print(\"[*] Generating dataset for R-CNN (object classification)\")\n",
    "        image_dir = self.dataset.train_dir / \"JPEGImages\"\n",
    "        annot_dir = self.dataset.train_dir / \"Annotations\"\n",
    "        object_counter = 0\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        pbar = tqdm(sorted(os.listdir(image_dir)), position=0, leave=True)\n",
    "        for img_name in pbar:\n",
    "            pbar.set_description(f\"Data size: {len(self.train_labels)}\")\n",
    "            \n",
    "            # load image and ground truth bounding boxes\n",
    "            img = cv2.imread(str(image_dir / img_name))\n",
    "            xml_path = annot_dir / img_name.replace(\".jpg\", \".xml\")\n",
    "            gt_bboxes = self.dataset.read_xml(xml_path)\n",
    "            \n",
    "            # directly use ground truth bboxes as positive samples\n",
    "            for gt_bbox in gt_bboxes:\n",
    "                cropped = img[gt_bbox['y1']:gt_bbox['y2'], gt_bbox['x1']:gt_bbox['x2'], :]\n",
    "                self.train_images.append(cropped)\n",
    "                gt_bbox_xywh = ((gt_bbox['x1']+gt_bbox['x2'])/2, (gt_bbox['y1']+gt_bbox['y2'])/2, \n",
    "                                (gt_bbox['x2']-gt_bbox['x1']), (gt_bbox['y2']-gt_bbox['y1']))\n",
    "                self.train_labels.append([gt_bbox['class'], gt_bbox_xywh, gt_bbox_xywh])\n",
    "            object_counter += len(gt_bboxes)  \n",
    "            \n",
    "            # collect background\n",
    "            if object_counter >= sample_ratio['object']:\n",
    "                object_counter -= sample_ratio['object']\n",
    "                background_counter = 0\n",
    "                \n",
    "                # generate bbox proposals with selective search\n",
    "                rects = selective_search(img)[:2000]  \n",
    "                random.shuffle(rects)\n",
    "                   \n",
    "                # loop through all proposals\n",
    "                for (x, y, w, h) in rects:\n",
    "                    # apply padding\n",
    "                    x1, x2 = np.clip([x - padding, x + w + padding], 0, img.shape[1])\n",
    "                    y1, y2 = np.clip([y - padding, y + h + padding], 0, img.shape[0])\n",
    "                    proposed_bbox = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2}\n",
    "                    is_object = False\n",
    "                    \n",
    "                    # check the proposal with every elements of the ground truth boxes\n",
    "                    for gt_bbox in gt_bboxes:\n",
    "                        iou = calculate_IoU(gt_bbox, proposed_bbox)\n",
    "                        if iou >= IoU_threshold['partial']:    # if object\n",
    "                            is_object = True\n",
    "                            break\n",
    "                        \n",
    "                    # save proposal if it's background\n",
    "                    if is_object == False:\n",
    "                        background_counter += 1\n",
    "                        cropped = img[y1 : y2, x1 : x2, :]\n",
    "                        self.train_images.append(cropped)\n",
    "                        proposed_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                        gt_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                        self.train_labels.append([0, proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                      \n",
    "                    # control the ration of object and background\n",
    "                    if background_counter == sample_ratio['background']:\n",
    "                        break\n",
    "        \n",
    "        print(\"[*] Dataset generated! Saving labels to \", self.data_path)\n",
    "        with open(self.data_path / \"train_images_classification.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_images, f)\n",
    "        with open(self.data_path / \"train_labels_classification.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RCNN_Classificaion_DataLoader(voc_dataset, cfg, train_cfg, shuffle=True):\n",
    "    rcnn_classification_dataset = RCNN_Classification_Dataset(voc_dataset, cfg)\n",
    "    return torch.utils.data.DataLoader(rcnn_classification_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=shuffle, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_classification_loader = RCNN_Classificaion_DataLoader(voc_dataset, config, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_classes = cfg['n_classes']\n",
    "        self.do_bbox_reg = cfg['bbox_reg']\n",
    "        self.max_proposals = cfg['max_proposals']\n",
    "        self.img_size = cfg['image_size']\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        print(\"[*] Initializing new network...\")\n",
    "        self.effnet = EfficientNet.from_pretrained('efficientnet-b0', num_classes=self.num_classes)\n",
    "        self.convnet = self.effnet.extract_features\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.AvgPool2d(7),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=False),\n",
    "            nn.Linear((1280, self.num_classes))\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        if self.do_bbox_reg:\n",
    "            self.bbox_reg = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear((1280, 4))\n",
    "            )\n",
    "\n",
    "\n",
    "    def refine_bbox(self, bbox, pred):\n",
    "        # bbox is list of [{x1, x2, y1, y2}, ...]\n",
    "        # pred is a sizr 4 array of predicted refinement of shape\n",
    "        x, y = (bbox['x1'] + bbox['x2']) / 2, (bbox['y1'] + bbox['y2']) / 2\n",
    "        w, h = bbox['x2'] - bbox['x1'], bbox['y2'] - bbox['y1']\n",
    "\n",
    "        new_x = x + w * pred[0]\n",
    "        new_y = y + h * pred[1]\n",
    "        new_w = w * torch.exp(pred[2])\n",
    "        new_h = h * torch.exp(pred[3])\n",
    "\n",
    "        return {'x1': new_x - new_w/2, 'x2': new_x + new_w/2, 'y1': new_y - new_h/2, 'y2': new_y + new_h/2}\n",
    "\n",
    "\n",
    "    def inference_single(self, img, rgb=False, batch_size=8, apply_nms=True, nms_threshold=0.2):\n",
    "        # img have to be loaded in BGR colorspace, or else rgb should be True\n",
    "        self.eval()\n",
    "        if rgb:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # perform selective search to find region proposals\n",
    "        rects = selective_search(img)\n",
    "        proposals = []\n",
    "        boxes = []\n",
    "        for (x, y, w, h) in rects[:self.max_proposals]:\n",
    "            roi = cv2.cvtColor(img[y:y+h, x:x+w, :], cv2.COLOR_RGB2BGR)\n",
    "            roi = preprocess(roi)\n",
    "            proposals.append(roi)\n",
    "            boxes.append({'x1':x, 'y1':y, 'x2':x+w, 'y2':y+h})\n",
    "        \n",
    "        # convert to dataloader for batching\n",
    "        proposals = torch.stack(proposals)\n",
    "        proposals = torch.Tensor(proposals)\n",
    "        proposals = torch.utils.data.TensorDataset(proposals)\n",
    "        proposals = torch.utils.data.DataLoader(proposals, batch_size=batch_size)\n",
    "\n",
    "        # predict probability of each box\n",
    "        bacth_iter = 0\n",
    "        useful_bboxes = []\n",
    "        for proposal_batch in tqdm(proposals):\n",
    "            patches = proposal_batch[0].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = self.convnet(patches)\n",
    "                features = self.flatten(features)\n",
    "                pred = self.classifier(features)\n",
    "                pred = self.softmax(pred)\n",
    "\n",
    "                if self.do_bbox_reg:\n",
    "                    bbox_refine = self.bbox_reg(features)\n",
    "            \n",
    "            # patches which are not classsified as background\n",
    "            useful_idx = torch.where(pred.argmax(1) > 0)    \n",
    "            for i in useful_idx[0]:\n",
    "                # loop through all useful patches\n",
    "                i = i.cpu().detach().numpy()\n",
    "                estimate = {}\n",
    "\n",
    "                class_prob = pred[i].cpu().detach().numpy()\n",
    "                estimate['class'] = class_prob.argmax(0)\n",
    "                estimate['confidence'] = class_prob.max(0)\n",
    "\n",
    "                original_bbox = boxes[bacth_iter * batch_size + i]\n",
    "                if self.do_bbox_reg == False:\n",
    "                    estimate['bbox'] = original_bbox\n",
    "                else:\n",
    "                    estimate['bbox'] = self.refine_bbox(original_bbox, bbox_refine[i])\n",
    "                \n",
    "                useful_bboxes.append(estimate)\n",
    "\n",
    "            bacth_iter += 1\n",
    "\n",
    "        # apply non-max suppression to remove duplicate boxes\n",
    "        if apply_nms:\n",
    "            useful_bboxes = nms(useful_bboxes, nms_threshold)\n",
    "        \n",
    "        return useful_bboxes\n",
    "    \n",
    "\n",
    "    def inference(self, imgs, rgb=False, batch_size=8, apply_nms=True, nms_threshold=0.2):\n",
    "        # when given single image\n",
    "        if type(imgs) == np.ndarray and len(imgs.shape) == 3:\n",
    "            return self.inference_single(imgs, rgb, batch_size, apply_nms)\n",
    "\n",
    "        bboxes = []\n",
    "        for img in tqdm(imgs):\n",
    "            pred_bboxes = self.inference_single(img, rgb, batch_size, apply_nms)\n",
    "            bboxes.append(pred_bboxes)\n",
    "        return bboxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efa59ba0514c88bea28add7efbea712f86beb100fe6589447027613f52b616e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
