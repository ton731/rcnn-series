{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/codex/implementing-r-cnn-object-detection-on-voc2012-with-pytorch-b05d3c623afe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from datetime import datetime\n",
    "import torchmetrics\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'image_size':224, 'n_classes':21, 'bbox_reg': True, 'network': 'efficientnet-b0', 'max_proposals':2000, 'pad': 16, 'confidence_threshold': 0.95}\n",
    "train_config = {'epochs': 1, 'batch_size':32, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5, 'ckpt_dir': Path('results')}\n",
    "load_path = None\n",
    "# load_path = Path(\"results/2023_01_10__11_12_25\")\n",
    "voc_2012_classes = ['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset:\n",
    "    def __init__(self):\n",
    "        self.root = Path(\"data\")\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "        self.train_dir = None\n",
    "        self.test_dir = None\n",
    "        self.train_data_link = None\n",
    "        self.test_data_link = None\n",
    "\n",
    "\n",
    "    def common_init(self):\n",
    "        # init for shared subclasses\n",
    "        self.label_type = ['none','aeroplane',\"Bicycle\",'bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'cow',\"Diningtable\",\"Dog\",\"Horse\",\"Motorbike\",'person', \"Pottedplant\",'sheep',\"Sofa\",\"Train\",\"TVmonitor\"]\n",
    "        self.convert_id = ['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]\n",
    "        self.convert_labels = {}\n",
    "        for i, x in enumerate(self.label_type):\n",
    "            self.convert_labels[x.lower()] = i\n",
    "\n",
    "        self.num_classes = len(self.label_type)     # 20 class + 1 background\n",
    "    \n",
    "\n",
    "    def download_dataset(self, validation_size=5000):\n",
    "        # download voc train dataset\n",
    "        if os.path.exists(self.root / \"voctrain.tar\"):\n",
    "            print(\"[*] Dataset already exists!\")\n",
    "        else:\n",
    "            print(\"[*] Downloading dataset...\")\n",
    "            print(self.train_data_link)\n",
    "            urllib.request.urlretrieve(self.train_data_link, self.root / \"voctrain.tar\")\n",
    "\n",
    "        if os.path.exists(self.root / \"VOCtrain\"):\n",
    "            print(\"[*] Dataset is already extracted!\")\n",
    "        else:\n",
    "            print(\"[*] Extracting dataset...\")\n",
    "            tar = tarfile.open(self.root / \"voctrain.tar\")\n",
    "            tar.extractall(self.root / \"VOCtrain\")\n",
    "            tar.close()\n",
    "\n",
    "        # download test dataset\n",
    "        if os.path.exists(self.root / \"VOCtest\"):\n",
    "            print(\"[*] Test dataset already exist!\")\n",
    "        else:\n",
    "            if self.test_data_link is None:\n",
    "                # move 5k images to validation set\n",
    "                print(\"[*] Moving validation data...\")\n",
    "                test_annotation_dir = self.test_dir / \"Annotations\"\n",
    "                test_img_dir = self.test_dir / \"JPEGImages\"\n",
    "                test_annotation_dir.mkdir(parents=True, exist_ok=True)\n",
    "                test_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                random.seed(731)\n",
    "                val_img_paths = random.sample(sorted(os.listdir(self.train_dir / \"JPEGImages\")), validation_size)\n",
    "\n",
    "                for path in val_img_paths:\n",
    "                    img_name = str(path).split(\"/\")[-1].split(\".\")[0]\n",
    "                    # move image\n",
    "                    os.rename(self.train_dir / \"JPEGImages\" / f\"{img_name}.jpg\", test_img_dir / f\"{img_name}.jpg\")\n",
    "                    # move annotation\n",
    "                    os.rename(self.train_dir / \"Annotations\" / f\"{img_name}.xml\", test_annotation_dir / f\"{img_name}.xml\")\n",
    "            else:\n",
    "                # load from val data\n",
    "                print(\"[*] Downloading validation dataset...\")\n",
    "                urllib.request.urlretrieve(self.test_data_link, \"voctset.tar\")\n",
    "\n",
    "                print(\"[*] Extracting validation dataset...\")\n",
    "                tar = tarfile.open(\"voctest.tar\", \"r:\")\n",
    "                tar.extractall(\"/content/VOCtest\")\n",
    "                tar.close()\n",
    "                # os.remove(\"/content/voctset.tar\")\n",
    "\n",
    "\n",
    "    def read_xml(self, xml_path):\n",
    "        object_list = []\n",
    "\n",
    "        tree = ET.parse(open(xml_path, 'r'))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        objects = root.findall(\"object\")\n",
    "        for _object in objects:\n",
    "            name = _object.find(\"name\").text\n",
    "            bndbox = _object.find(\"bndbox\")\n",
    "            xmin = int(bndbox.find(\"xmin\").text)\n",
    "            ymin = int(bndbox.find(\"ymin\").text)\n",
    "            xmax = int(bndbox.find(\"xmax\").text)\n",
    "            ymax = int(bndbox.find(\"ymax\").text)\n",
    "            class_name = _object.find(\"name\").text\n",
    "            object_list.append({'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax, 'class':self.convert_labels[class_name]})\n",
    "        \n",
    "        return object_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007(VOCDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_dir = self.root / 'VOCtrain/VOCdevkit/VOC2007'\n",
    "        self.test_dir = self.root / 'VOCtest/VOCdevkit/VOC2007'\n",
    "        self.train_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
    "        self.test_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar'\n",
    "        self.common_init()  #mandatory\n",
    "    \n",
    "class VOC2012(VOCDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_dir = self.root / 'VOCtrain/VOCdevkit/VOC2012'\n",
    "        self.test_dir = self.root / 'VOCtest/VOCdevkit/VOC2012'\n",
    "        # original site goes down frequently, so we use a link to the clone alternatively\n",
    "        # self.train_data_link = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar' \n",
    "        self.train_data_link = 'http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar'\n",
    "        self.test_data_link = None\n",
    "        self.common_init()  #mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Dataset already exists!\n",
      "[*] Dataset is already extracted!\n",
      "[*] Test dataset already exist!\n"
     ]
    }
   ],
   "source": [
    "voc_dataset = VOC2012()\n",
    "voc_dataset.download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data num: 12125\n",
      "valid data num: 5000\n"
     ]
    }
   ],
   "source": [
    "train_data_num = len(os.listdir(voc_dataset.train_dir / \"Annotations\"))\n",
    "valid_data_num = len(os.listdir(voc_dataset.test_dir / \"Annotations\"))\n",
    "print(\"train data num:\", train_data_num)\n",
    "print(\"valid data num:\", valid_data_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img_bgr):\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_img_from_path(img_path):\n",
    "    print(\"plotting:\", img_path)\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    print(\"img shape:\", img_rgb.shape)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_bounding_box_from_path(img_path):\n",
    "    # img\n",
    "    print(\"plotting:\", img_path)\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    xml_path = img_path.replace(\"JPEGImages\", \"Annotations\").replace(\".jpg\", \".xml\")\n",
    "    tree = ET.parse(open(xml_path, 'r'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # print image shape\n",
    "    w, h = root.find(\"size\").find(\"width\").text, root.find(\"size\").find(\"height\").text\n",
    "    print(\"width, height:\", w, h)\n",
    "\n",
    "    # bounding box settings\n",
    "    box_img = img_bgr.copy()\n",
    "    bbox_color = (0, 69, 255)    # bgr\n",
    "    bbox_thickness = 2\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_color = bbox_color\n",
    "    line_type = 1\n",
    "\n",
    "    # plot box\n",
    "    objects = root.findall(\"object\")\n",
    "    for _object in objects:\n",
    "        name = _object.find(\"name\").text\n",
    "        bndbox = _object.find(\"bndbox\")\n",
    "        xmin = int(bndbox.find(\"xmin\").text)\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "        class_name = _object.find('name').text\n",
    "\n",
    "        cv2.rectangle(box_img, (xmin, ymin), (xmax, ymax), bbox_color, bbox_thickness)\n",
    "        cv2.putText(box_img, class_name, (xmin, ymin-5), font, font_scale, font_color, line_type)\n",
    "\n",
    "    box_img_rgb = cv2.cvtColor(box_img, cv2.COLOR_BGR2RGB)\n",
    "    result = np.hstack((img_rgb, box_img_rgb))\n",
    "    # plt.imshow(result)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "    plt.imshow(box_img_rgb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimg_name = random.choice(os.listdir(voc_dataset.train_dir / \"JPEGImages\"))\\nimg_path = str(voc_dataset.train_dir / \"JPEGImages\" / img_name)\\nplot_bounding_box_from_path(img_path)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "img_name = random.choice(os.listdir(voc_dataset.train_dir / \"JPEGImages\"))\n",
    "img_path = str(voc_dataset.train_dir / \"JPEGImages\" / img_name)\n",
    "plot_bounding_box_from_path(img_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img):\n",
    "    # return region proposals of selective searh over an image\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    return rects\n",
    "\n",
    "\n",
    "def plot_results(img, bboxes, color=(0, 69, 255)):\n",
    "    plot_cfg = {'bbox_color':color, 'bbox_thickness':2, \n",
    "                'font':cv2.FONT_HERSHEY_SIMPLEX, 'font_scale':0.5, 'font_color':color, 'line_thickness':1}\n",
    "    img_bb = img.copy()\n",
    "    for box in bboxes:\n",
    "        bbox = box['bbox']\n",
    "        cv2.rectangle(img_bb, (bbox['x1'], bbox['y1']), (bbox['x2'], bbox['y2']), plot_cfg['bbox_color'], plot_cfg['bbox_thickness'])\n",
    "        cv2.putText(img_bb, f\"{voc_dataset.label_type[box['class']]}, {str(box['conf'])[:5]}\",  (bbox['x1'], bbox['y1'] - 5), plot_cfg['font'], \n",
    "                    plot_cfg['font_scale'], plot_cfg['font_color'], plot_cfg['line_thickness'])\n",
    "    return img_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimg_path = str(voc_dataset.train_dir / \"JPEGImages\" / img_name)\\nimg = cv2.imread(img_path)\\nrects = selective_search(img)\\nfor i, rect in enumerate(rects):\\n    if i > 2000:\\n        break\\n    x, y, w, h = rect\\n    cv2.rectangle(img, (x, y), (x+w, y+h), (100, 255, 100), 1)\\nplot_img(img)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "img_path = str(voc_dataset.train_dir / \"JPEGImages\" / img_name)\n",
    "img = cv2.imread(img_path)\n",
    "rects = selective_search(img)\n",
    "for i, rect in enumerate(rects):\n",
    "    if i > 2000:\n",
    "        break\n",
    "    x, y, w, h = rect\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (100, 255, 100), 1)\n",
    "plot_img(img)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU (Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IoU(bb1, bb2):\n",
    "    # IoU = area_of_overlap / area_of_union\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "    \n",
    "    # return IoU as 0 if 2 boxes are not intersected\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    # calculate areas\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "    union_area = bb1_area + bb2_area - intersection_area\n",
    "\n",
    "    # iou\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my iou: 0.07317073170731707\n"
     ]
    }
   ],
   "source": [
    "bb1 = {'x1':100, 'x2':200, 'y1':50, 'y2':150}\n",
    "bb2 = {'x1':150, 'x2':300, 'y1':120, 'y2':200}\n",
    "print(\"my iou:\", calculate_IoU(bb1, bb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mAP (Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(pred, truth, iou_threshold=0.5, num_classes=21, per_class=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMS (Non-max suppression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes, iou_threshold=0.5):\n",
    "    # bboxes: list of dicts {'bbox':(x1,x2,y1,y2), 'confidence':float, 'class':int}\n",
    "    confidence_list = np.array([bbox['confidence'] for bbox in bboxes])\n",
    "    confidence_order = (-confidence_list).argsort()   # apply minus to make the order descending\n",
    "    is_removed = [False for _ in range(len(bboxes))]\n",
    "    selected_bboxes = []\n",
    "    \n",
    "    for i in range(len(bboxes)):\n",
    "        bbox_idx = confidence_order[i]\n",
    "        if is_removed[bbox_idx]:\n",
    "            continue\n",
    "        \n",
    "        # add bbox to the main bbox object\n",
    "        selected_bboxes.append(bboxes[bbox_idx])\n",
    "        is_removed[bbox_idx] = True\n",
    "        \n",
    "        # remove overlapping bboxes\n",
    "        for order in range(i+1, len(bboxes)):\n",
    "            other_bbox_idx = confidence_order[order]\n",
    "            # check if the bbox not remove yet\n",
    "            if is_removed[other_bbox_idx] == False:\n",
    "                # check overlapping\n",
    "                iou = calculate_IoU(bboxes[bbox_idx]['bbox'], bboxes[other_bbox_idx]['bbox'])\n",
    "                if iou >= iou_threshold:\n",
    "                    is_removed[other_bbox_idx] = True\n",
    "    \n",
    "    return selected_bboxes\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Domain-specific fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    It's for domain-specific fine-tuning, inputs are the cropped image of the bounding\n",
    "    boxes, and outputs are the labels of the cropped images, such as background, class 1,\n",
    "    class 2, ... class N.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, cfg, IoU_threshold={'positive':0.5, 'partial':0.3},\n",
    "                 sample_ratio={'object':32, 'background':96}, data_path=Path(\"data/domain-specific\")):\n",
    "        self.data_path = data_path\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if self.dataset_exists() == False:\n",
    "            self.generate_dataset(sample_ratio, IoU_threshold)\n",
    "        else:\n",
    "            print(\"[*] Loading domain-specific dataset from\", self.data_path)\n",
    "            with open(self.data_path / \"train_images.pkl\", 'rb') as f:\n",
    "                self.train_images = pickle.load(f)\n",
    "            with open(self.data_path / \"train_labels.pkl\", 'rb') as f:\n",
    "                self.train_labels = pickle.load(f)\n",
    "                \n",
    "            # check if both files are complete and flawless\n",
    "            if not len(self.train_images) == len(self.train_labels):\n",
    "                raise ValueError(\"The loaded dataset is invalid or in different size.\")\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if torch.is_tensor(i):\n",
    "            i = i.tolist()\n",
    "        img = Image.fromarray(cv2.cvtColor(self.train_images[i], cv2.COLOR_BGR2RGB))\n",
    "        return {'image': self.transform(img), 'label': self.train_labels[i][0],\n",
    "                'proposed_bbox': self.train_labels[i][1], 'gt_bbox': self.train_labels[i][2]}\n",
    "        \n",
    "        \n",
    "    def dataset_exists(self):\n",
    "        if os.path.exists(self.data_path / \"train_images.pkl\") == False:\n",
    "            return False\n",
    "        if os.path.exists(self.data_path / \"train_labels.pkl\") == False:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def generate_dataset(self, sample_ratio, IoU_threshold, padding=16):\n",
    "        print(\"[*] Generating dataset for R-CNN\")\n",
    "        image_dir = self.dataset.train_dir / \"JPEGImages\"\n",
    "        annot_dir = self.dataset.train_dir / \"Annotations\"\n",
    "        object_counter = 0\n",
    "        background_counter = 0\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        pbar = tqdm(sorted(os.listdir(image_dir))[:2000], position=0, leave=True)\n",
    "        for img_name in pbar:\n",
    "            pbar.set_description(f\"Data size: {len(self.train_labels)}\")\n",
    "            \n",
    "            # load image and ground truth bounding boxes\n",
    "            img = cv2.imread(str(image_dir / img_name))\n",
    "            xml_path = annot_dir / img_name.replace(\".jpg\", \".xml\")\n",
    "            gt_bboxes = self.dataset.read_xml(xml_path)\n",
    "            \n",
    "            # generate bounding box proposals from selective search\n",
    "            rects = selective_search(img)[:2000]  # use only 2000 box\n",
    "            random.shuffle(rects)\n",
    "            \n",
    "            # loop through all proposals\n",
    "            for (x, y, w, h) in rects:\n",
    "                # apply padding\n",
    "                x1, x2 = np.clip([x - padding, x + w + padding], 0, img.shape[1])\n",
    "                y1, y2 = np.clip([y - padding, y + h + padding], 0, img.shape[0])\n",
    "                proposed_bbox = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2}\n",
    "                \n",
    "                # check the proposal with every elements of the ground truth boxes\n",
    "                is_object = False   # define flag\n",
    "                for gt_bbox in gt_bboxes:\n",
    "                    iou = calculate_IoU(gt_bbox, proposed_bbox)\n",
    "                    if iou >= IoU_threshold['positive']:    # iou >= 0.5\n",
    "                        object_counter += 1\n",
    "                        # add image\n",
    "                        cropped = img[y1 : y2, x1 : x2, :]\n",
    "                        self.train_images.append(cropped)\n",
    "                        # add label, here x, y is the center of the rectangle\n",
    "                        proposed_bbox_xywh = ((x1+x2)/2, (y1+y2)/2, (x2-x1), (y2-y1))\n",
    "                        gt_bbox_xywh = ((gt_bbox['x1']+gt_bbox['x2'])/2, (gt_bbox['y1']+gt_bbox['y2'])/2, \n",
    "                                        (gt_bbox['x2']-gt_bbox['x1']), (gt_bbox['y2']-gt_bbox['y1']))\n",
    "                        self.train_labels.append([gt_bbox['class'], proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                        \n",
    "                        is_object = True\n",
    "                        break\n",
    "                \n",
    "                # if the proposal is not close to any ground truth box\n",
    "                if background_counter < sample_ratio['background'] and is_object == False:\n",
    "                    background_counter += 1\n",
    "                    # add background image\n",
    "                    cropped = img[y1 : y2, x1 : x2, :]\n",
    "                    self.train_images.append(cropped)\n",
    "                    # add background label\n",
    "                    proposed_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                    gt_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                    self.train_labels.append([0, proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                \n",
    "                # control the ratio between object and backgruond\n",
    "                if object_counter >= sample_ratio['object'] and background_counter == sample_ratio['background']:\n",
    "                    object_counter -= sample_ratio['object']\n",
    "                    background_counter = 0\n",
    "        \n",
    "        print(\"[*] Dataset generated! Saving labels to \", self.data_path)\n",
    "        with open(self.data_path / \"train_images.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_images, f)\n",
    "        with open(self.data_path / \"train_labels.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RCNN_DataLoader(voc_dataset, cfg, train_cfg, shuffle=True):\n",
    "    rcnn_dataset = RCNN_Dataset(voc_dataset, cfg)\n",
    "    return torch.utils.data.DataLoader(rcnn_dataset, batch_size=train_cfg['batch_size'], shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading domain-specific dataset from data\\domain-specific\n"
     ]
    }
   ],
   "source": [
    "rcnn_loader = RCNN_DataLoader(voc_dataset, config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninverse_transform = transforms.Compose([\\n    transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\\n    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\\n    transforms.ToPILImage()\\n])\\n\\nfor x in rcnn_loader:\\n    for i in range(5):\\n        plt.imshow(inverse_transform(x['image'][i]))\\n        plt.title(voc_2012_classes[x['label'][i]])\\n        plt.show()\\n    break\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "for x in rcnn_loader:\n",
    "    for i in range(5):\n",
    "        plt.imshow(inverse_transform(x['image'][i]))\n",
    "        plt.title(voc_2012_classes[x['label'][i]])\n",
    "        plt.show()\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Object category classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Classification_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    It's for the final object category classification model. The positive samples\n",
    "    are from the ground truth bboxes, and the negative samples are from the bbox\n",
    "    proposed with IoU smaller than 0.3.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, cfg, IoU_threshold={'positive':0.5, 'partial':0.3},\n",
    "                 sample_ratio={'object':32, 'background':96}, data_path=Path(\"data/object-classification\")):\n",
    "        self.data_path = data_path\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if self.dataset_exists() == False:\n",
    "            self.generate_dataset(sample_ratio, IoU_threshold)\n",
    "        else:\n",
    "            print(\"[*] Loading object-classification dataset from\", self.data_path)\n",
    "            with open(self.data_path / \"train_images_classification.pkl\", 'rb') as f:\n",
    "                self.train_images = pickle.load(f)\n",
    "            with open(self.data_path / \"train_labels_classification.pkl\", 'rb') as f:\n",
    "                self.train_labels = pickle.load(f)\n",
    "                \n",
    "            # check if both files are complete and flawless\n",
    "            if not len(self.train_images) == len(self.train_labels):\n",
    "                raise ValueError(\"The loaded dataset is invalid or in different size.\")\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if torch.is_tensor(i):\n",
    "            i = i.tolist()\n",
    "        img = Image.fromarray(cv2.cvtColor(self.train_images[i], cv2.COLOR_BGR2RGB))\n",
    "        return {'image': self.transform(img), 'label': self.train_labels[i][0],\n",
    "                'proposed_bbox': self.train_labels[i][1], 'gt_bbox': self.train_labels[i][2]}\n",
    "        \n",
    "        \n",
    "    def dataset_exists(self):\n",
    "        if os.path.exists(self.data_path / \"train_images_classification.pkl\") == False:\n",
    "            return False\n",
    "        if os.path.exists(self.data_path / \"train_labels_classification.pkl\") == False:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def generate_dataset(self, sample_ratio, IoU_threshold, padding=16):\n",
    "        print(\"[*] Generating dataset for R-CNN (object classification)\")\n",
    "        image_dir = self.dataset.train_dir / \"JPEGImages\"\n",
    "        annot_dir = self.dataset.train_dir / \"Annotations\"\n",
    "        object_counter = 0\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        pbar = tqdm(sorted(os.listdir(image_dir)), position=0, leave=True)\n",
    "        for img_name in pbar:\n",
    "            pbar.set_description(f\"Data size: {len(self.train_labels)}\")\n",
    "            \n",
    "            # load image and ground truth bounding boxes\n",
    "            img = cv2.imread(str(image_dir / img_name))\n",
    "            xml_path = annot_dir / img_name.replace(\".jpg\", \".xml\")\n",
    "            gt_bboxes = self.dataset.read_xml(xml_path)\n",
    "            \n",
    "            # directly use ground truth bboxes as positive samples\n",
    "            for gt_bbox in gt_bboxes:\n",
    "                cropped = img[gt_bbox['y1']:gt_bbox['y2'], gt_bbox['x1']:gt_bbox['x2'], :]\n",
    "                self.train_images.append(cropped)\n",
    "                gt_bbox_xywh = ((gt_bbox['x1']+gt_bbox['x2'])/2, (gt_bbox['y1']+gt_bbox['y2'])/2, \n",
    "                                (gt_bbox['x2']-gt_bbox['x1']), (gt_bbox['y2']-gt_bbox['y1']))\n",
    "                self.train_labels.append([gt_bbox['class'], gt_bbox_xywh, gt_bbox_xywh])\n",
    "            object_counter += len(gt_bboxes)  \n",
    "            \n",
    "            # collect background\n",
    "            if object_counter >= sample_ratio['object']:\n",
    "                object_counter -= sample_ratio['object']\n",
    "                background_counter = 0\n",
    "                \n",
    "                # generate bbox proposals with selective search\n",
    "                rects = selective_search(img)[:2000]  \n",
    "                random.shuffle(rects)\n",
    "                   \n",
    "                # loop through all proposals\n",
    "                for (x, y, w, h) in rects:\n",
    "                    # apply padding\n",
    "                    x1, x2 = np.clip([x - padding, x + w + padding], 0, img.shape[1])\n",
    "                    y1, y2 = np.clip([y - padding, y + h + padding], 0, img.shape[0])\n",
    "                    proposed_bbox = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2}\n",
    "                    is_object = False\n",
    "                    \n",
    "                    # check the proposal with every elements of the ground truth boxes\n",
    "                    for gt_bbox in gt_bboxes:\n",
    "                        iou = calculate_IoU(gt_bbox, proposed_bbox)\n",
    "                        if iou >= IoU_threshold['partial']:    # if object\n",
    "                            is_object = True\n",
    "                            break\n",
    "                        \n",
    "                    # save proposal if it's background\n",
    "                    if is_object == False:\n",
    "                        background_counter += 1\n",
    "                        cropped = img[y1 : y2, x1 : x2, :]\n",
    "                        self.train_images.append(cropped)\n",
    "                        proposed_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                        gt_bbox_xywh = (1.0, 1.0, 1.0, 1.0)\n",
    "                        self.train_labels.append([0, proposed_bbox_xywh, gt_bbox_xywh])\n",
    "                      \n",
    "                    # control the ration of object and background\n",
    "                    if background_counter == sample_ratio['background']:\n",
    "                        break\n",
    "        \n",
    "        print(\"[*] Dataset generated! Saving labels to \", self.data_path)\n",
    "        with open(self.data_path / \"train_images_classification.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_images, f)\n",
    "        with open(self.data_path / \"train_labels_classification.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RCNN_Classificaion_DataLoader(voc_dataset, cfg, train_cfg, shuffle=True):\n",
    "    rcnn_classification_dataset = RCNN_Classification_Dataset(voc_dataset, cfg)\n",
    "    return torch.utils.data.DataLoader(rcnn_classification_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading object-classification dataset from data\\object-classification\n"
     ]
    }
   ],
   "source": [
    "rcnn_classification_loader = RCNN_Classificaion_DataLoader(voc_dataset, config, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_classes = cfg['n_classes']\n",
    "        self.do_bbox_reg = cfg['bbox_reg']\n",
    "        self.max_proposals = cfg['max_proposals']\n",
    "        self.img_size = cfg['image_size']\n",
    "        self.confidence_threshold = cfg['confidence_threshold']\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        print(\"[*] Initializing new network...\")\n",
    "        self.effnet = EfficientNet.from_pretrained('efficientnet-b0', num_classes=self.num_classes)\n",
    "        self.convnet = self.effnet.extract_features\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.AvgPool2d(7),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=False),\n",
    "            nn.Linear(1280, self.num_classes)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        if self.do_bbox_reg:\n",
    "            self.bbox_reg = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=False),\n",
    "                nn.Linear(1280, 4)\n",
    "            )\n",
    "\n",
    "\n",
    "    def refine_bbox(self, bbox, pred):\n",
    "        # bbox is list of [{x1, x2, y1, y2}, ...]\n",
    "        # pred is a sizr 4 array of predicted refinement of shape\n",
    "        x, y = (bbox['x1'] + bbox['x2']) / 2, (bbox['y1'] + bbox['y2']) / 2\n",
    "        w, h = bbox['x2'] - bbox['x1'], bbox['y2'] - bbox['y1']\n",
    "\n",
    "        new_x = x + w * pred[0]\n",
    "        new_y = y + h * pred[1]\n",
    "        new_w = w * torch.exp(pred[2])\n",
    "        new_h = h * torch.exp(pred[3])\n",
    "\n",
    "        return {'x1': new_x - new_w/2, 'x2': new_x + new_w/2, 'y1': new_y - new_h/2, 'y2': new_y + new_h/2}\n",
    "\n",
    "\n",
    "    def inference_single(self, img, rgb=False, batch_size=8, apply_nms=True, nms_threshold=0.2):\n",
    "        # img have to be loaded in BGR colorspace, or else rgb should be True\n",
    "        self.eval()\n",
    "        if rgb:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # perform selective search to find region proposals\n",
    "        rects = selective_search(img)\n",
    "        proposals = []\n",
    "        boxes = []\n",
    "        for (x, y, w, h) in rects[:self.max_proposals]:\n",
    "            roi = cv2.cvtColor(img[y:y+h, x:x+w, :], cv2.COLOR_RGB2BGR)\n",
    "            roi = preprocess(roi)\n",
    "            proposals.append(roi)\n",
    "            boxes.append({'x1':x, 'y1':y, 'x2':x+w, 'y2':y+h})\n",
    "        \n",
    "        # convert to dataloader for batching\n",
    "        proposals = torch.stack(proposals)\n",
    "        proposals = torch.Tensor(proposals)\n",
    "        proposals = torch.utils.data.TensorDataset(proposals)\n",
    "        proposals = torch.utils.data.DataLoader(proposals, batch_size=batch_size)\n",
    "\n",
    "        # predict probability of each box\n",
    "        bacth_iter = 0\n",
    "        useful_bboxes = []\n",
    "        for proposal_batch in tqdm(proposals):\n",
    "            patches = proposal_batch[0].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = self.convnet(patches)\n",
    "                features = self.flatten(features)\n",
    "                pred = self.classifier(features)\n",
    "                pred = self.softmax(pred)\n",
    "\n",
    "                if self.do_bbox_reg:\n",
    "                    bbox_refine = self.bbox_reg(features)\n",
    "            \n",
    "            # patches which are not classsified as background\n",
    "            useful_idx = torch.where(pred.argmax(1) > 0)    \n",
    "            for i in useful_idx[0]:\n",
    "                # loop through all useful patches\n",
    "                i = i.cpu().detach().numpy()\n",
    "                estimate = {}\n",
    "\n",
    "                class_prob = pred[i].cpu().detach().numpy()\n",
    "                estimate['class'] = class_prob.argmax(0)\n",
    "                estimate['confidence'] = class_prob.max(0)\n",
    "\n",
    "                original_bbox = boxes[bacth_iter * batch_size + i]\n",
    "                if self.do_bbox_reg == False:\n",
    "                    estimate['bbox'] = original_bbox\n",
    "                else:\n",
    "                    estimate['bbox'] = self.refine_bbox(original_bbox, bbox_refine[i])\n",
    "                \n",
    "                if estimate['confidence'] > self.confidence_threshold:\n",
    "                    useful_bboxes.append(estimate)\n",
    "\n",
    "            bacth_iter += 1\n",
    "\n",
    "        # apply non-max suppression to remove duplicate boxes\n",
    "        if apply_nms:\n",
    "            useful_bboxes = nms(useful_bboxes, nms_threshold)\n",
    "\n",
    "        # convert bboxes values to numpy\n",
    "        for i in range(len(useful_bboxes)):\n",
    "            useful_bboxes[i][\"bbox\"][\"x1\"] = int(useful_bboxes[i][\"bbox\"][\"x1\"].cpu().numpy())\n",
    "            useful_bboxes[i][\"bbox\"][\"y1\"] = int(useful_bboxes[i][\"bbox\"][\"y1\"].cpu().numpy())\n",
    "            useful_bboxes[i][\"bbox\"][\"x2\"] = int(useful_bboxes[i][\"bbox\"][\"x2\"].cpu().numpy())\n",
    "            useful_bboxes[i][\"bbox\"][\"y2\"] = int(useful_bboxes[i][\"bbox\"][\"y2\"].cpu().numpy())\n",
    "\n",
    "        \n",
    "        return useful_bboxes\n",
    "    \n",
    "\n",
    "    def inference(self, imgs, rgb=False, batch_size=8, apply_nms=True, nms_threshold=0.2):\n",
    "        # when given single image\n",
    "        if type(imgs) == np.ndarray and len(imgs.shape) == 3:\n",
    "            return self.inference_single(imgs, rgb, batch_size, apply_nms)\n",
    "\n",
    "        bboxes = []\n",
    "        for img in tqdm(imgs):\n",
    "            pred_bboxes = self.inference_single(img, rgb, batch_size, apply_nms)\n",
    "            bboxes.append(pred_bboxes)\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_Trainer:\n",
    "\tdef __init__(self, model, fine_tuning_loader, category_classification_loader, train_cfg):\n",
    "\t\tself.model = model\n",
    "\t\tself.fine_tuning_loader = fine_tuning_loader\n",
    "\t\tself.category_classification_loader = category_classification_loader\n",
    "\t\tself.train_cfg = train_cfg\n",
    "\n",
    "\t\tself._init_env()\n",
    "\t\t\n",
    "\t\n",
    "\tdef _init_env(self):\n",
    "\t\t# checkpoint directory\n",
    "\t\tckpt_dir = self.train_cfg['ckpt_dir']\n",
    "\t\tckpt_dir = ckpt_dir / f'{datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")}'\n",
    "\t\tckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\t\tself.ckpt_dir = ckpt_dir\n",
    "\t \n",
    "\t\t# logger\n",
    "\t\tlogger = logging.getLogger(name='RCNN')\n",
    "\t\tlogger.setLevel(level=logging.INFO)\n",
    "\t\t# set formatter\n",
    "\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\t\t# console handler\n",
    "\t\tstream_handler = logging.StreamHandler()\n",
    "\t\tstream_handler.setFormatter(formatter)\n",
    "\t\tlogger.addHandler(stream_handler)\n",
    "\t\t# file handler\n",
    "\t\tfile_handler = logging.FileHandler(os.path.join(ckpt_dir, \"record.log\"))\n",
    "\t\tfile_handler.setFormatter(formatter)\n",
    "\t\tlogger.addHandler(file_handler)\n",
    "\t\t\n",
    "\t\tself.logger = logger\n",
    "\t\tself.logger.info(self.train_cfg)\n",
    "\t\n",
    "\t\n",
    "\tdef fine_tune_training(self):\n",
    "\t\t# define loss function\n",
    "\t\tcriterion_ce = nn.CrossEntropyLoss()\n",
    "\t\tcriterion_mse = nn.MSELoss()\n",
    "\t\t\n",
    "\t\t# compute the gradients for the entire model\n",
    "\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=self.train_cfg['lr'], weight_decay=self.train_cfg['l2_reg'])\n",
    "\t\t\n",
    "\t\t# lr scheduler\n",
    "\t\tscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.train_cfg['lr_decay'])\n",
    "\t\t\n",
    "\t\t# initialize metrics\n",
    "\t\taccuracy_counter = torchmetrics.Accuracy(task=\"multiclass\", num_classes=21)\n",
    "\t\t\n",
    "\t\t# train loop\n",
    "\t\tbest_train_loss = np.inf\n",
    "\t\tfor epoch in range(self.train_cfg['epochs']):\n",
    "\t\t\tself.model.train()\n",
    "\t\t\tprint(f\"[*] Training fine-tuning epoch {epoch+1}/{self.train_cfg['epochs']}\")\n",
    "\t\t\tpbar = tqdm(self.fine_tuning_loader)\n",
    "\t\t\ttrain_loss = 0\n",
    "\t\t\ttrain_acc = 0\n",
    "\t\t\ttrain_iter = 0\n",
    "\t\t\tfor step, batch in enumerate(pbar):\n",
    "\t\t\t\tfeatures = self.model.convnet(batch['image'].to(device))\n",
    "\t\t\t\tfeatures = self.model.flatten(features)\n",
    "\t\t\t\toutput = self.model.classifier(features)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# loss\n",
    "\t\t\t\tclf_loss = criterion_ce(output, batch['label'].to(device))\n",
    "\t\t\t\tloss = clf_loss\n",
    "\t\t\t\t\n",
    "\t\t\t\t# bbox regression loss\n",
    "\t\t\t\tif self.model.do_bbox_reg:\n",
    "\t\t\t\t\tbbox_est = self.model.bbox_reg(features)\n",
    "\t\t\t\t\tp_x, p_y, p_w, p_h = batch['proposed_bbox'][0], batch['proposed_bbox'][1], batch['proposed_bbox'][2], batch['proposed_bbox'][3]\n",
    "\t\t\t\t\tg_x, g_y, g_w, g_h = batch['gt_bbox'][0], batch['gt_bbox'][1], batch['gt_bbox'][2], batch['gt_bbox'][3]\n",
    "\n",
    "\t\t\t\t\tbbox_ans = torch.stack([(g_x-p_x)/p_w, (g_y-p_y)/p_h, torch.log(g_w)/p_w, torch.log(g_h)/p_h], axis=1)\n",
    "\t\t\t\t\tbbox_ans = bbox_ans.float().to(device)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# count only bboxes that are not backgrond\n",
    "\t\t\t\t\tnot_bg = (batch['label']>0).reshape(len(batch['label']), 1).to(device)\n",
    "\t\t\t\t\tbbox_est = bbox_est * not_bg\n",
    "\t\t\t\t\tbbox_ans = bbox_ans * not_bg\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# add to loss\n",
    "\t\t\t\t\tbbox_loss = criterion_mse(bbox_est, bbox_ans)\n",
    "\t\t\t\t\tloss += bbox_loss\n",
    "\n",
    "\t\t\t\t# backpropagation\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\ttrain_loss += loss.cpu().detach().numpy()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# logging\n",
    "\t\t\t\tacc = accuracy_counter(output.cpu(), batch['label'])\n",
    "\t\t\t\ttrain_acc += acc.numpy()\n",
    "\t\t\t\ttrain_iter += 1\n",
    "\t\t\t\tpbar.set_description(f\"loss: {loss.cpu().detach().numpy():3f},  accuracy: {acc.numpy():.3f}\")\n",
    "\t\t\tself.logger.info(f\"Epoch: {epoch+1}, loss: {train_loss:3f},  accuracy: {train_acc/train_iter:.3f}\")\n",
    "\t\t\t\n",
    "\t\t\t# save model\n",
    "\t\t\tif train_loss < best_train_loss:\n",
    "\t\t\t\tbest_train_loss = train_loss\n",
    "\t\t\t\ttorch.save(self.model.state_dict(), self.ckpt_dir / \"model.pt\")\n",
    "\t\t\t\n",
    "\t\t\t# update lr\n",
    "\t\t\tscheduler.step()\n",
    "\t\t\n",
    "\t\n",
    "\tdef category_classification_training(self):\n",
    "\t\t# define loss functionn\n",
    "\t\tcriterion_ce = nn.CrossEntropyLoss()\n",
    "\t\t\n",
    "\t\t# gradiens for final classifier only\n",
    "\t\toptimizer = torch.optim.Adam(self.model.classifier.parameters(), lr=self.train_cfg['lr'], weight_decay=self.train_cfg['l2_reg'])\n",
    "\t\t\n",
    "\t\t# initialize metrics\n",
    "\t\taccuracy_counter = torchmetrics.Accuracy(task=\"multiclass\", num_classes=21)\n",
    "\t\t\n",
    "\t\t# train loop\n",
    "\t\tbest_train_loss = np.inf\n",
    "\t\tfor epoch in range(self.train_cfg['epochs']):\n",
    "\t\t\tself.model.train()\n",
    "\t\t\tprint(f\"[*] Training category-classification epoch {epoch+1}/{self.train_cfg['epochs']}\")\n",
    "\t\t\tpbar = tqdm(self.category_classification_loader)\n",
    "\t\t\ttrain_loss = 0\n",
    "\t\t\ttrain_acc = 0\n",
    "\t\t\ttrain_iter = 0\n",
    "\t\t\tfor step, batch in enumerate(pbar):\n",
    "\t\t\t\tfeatures = self.model.convnet(batch['image'].to(device))\n",
    "\t\t\t\tfeatures = self.model.flatten(features)\n",
    "\t\t\t\toutput = self.model.classifier(features)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# backpropagation\n",
    "\t\t\t\tclf_loss = criterion_ce(output, batch['label'].to(device))\n",
    "\t\t\t\tloss = clf_loss\n",
    "\t\t\t\ttrain_loss += loss.cpu().detach().numpy()\n",
    "\t\t\t\t\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# logging\n",
    "\t\t\t\tacc = accuracy_counter(output.cpu(), batch['label'])\n",
    "\t\t\t\ttrain_acc += acc.numpy()\n",
    "\t\t\t\ttrain_iter += 1\n",
    "\t\t\t\tpbar.set_description(f\"loss: {loss.cpu().detach().numpy():3f},  accuracy: {acc.numpy():.3f}\")\n",
    "\t\t\tself.logger.info(f\"Epoch: {epoch+1}, loss: {train_loss:3f},  accuracy: {train_acc/train_iter:.3f}\")\n",
    "\t\t\t\n",
    "\t\t\t# save model\n",
    "\t\t\tif train_loss < best_train_loss:\n",
    "\t\t\t\tbest_train_loss = train_loss\n",
    "\t\t\t\ttorch.save(self.model.state_dict(), self.ckpt_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing new network...\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 12:48:12,164 - RCNN - INFO - {'epochs': 1, 'batch_size': 32, 'lr': 0.001, 'lr_decay': 0.5, 'l2_reg': 1e-05, 'ckpt_dir': WindowsPath('results')}\n"
     ]
    }
   ],
   "source": [
    "model = RCNN(config).to(device)\n",
    "if load_path:\n",
    "    model.load_state_dict(torch.load(load_path / \"model.pt\"))\n",
    "trainer = RCNN_Trainer(model, rcnn_loader, rcnn_classification_loader, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fine_tune_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.category_classification_training()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize(voc_dataset, model, img_path, i, save_to_disk=False):\n",
    "    img = cv2.imread(img_path)\n",
    "    pred_bboxes = model.inference(img, apply_nms=True, nms_threshold=0.2)\n",
    "    pred_bboxes=nms(pred_bboxes, 0.2)\n",
    "    # plot predicted\n",
    "    # print(\"Predicted:\")\n",
    "    predcited_name = f\"{i}_pred.png\"\n",
    "    plot_bboxes(img, pred_bboxes, predcited_name, save_to_disk)\n",
    "    # plot ground truth\n",
    "    xml_path = img_path.replace(\"JPEGImages\", \"Annotations\").replace(\".jpg\", \".xml\")\n",
    "    gt_bboxes = read_gt_xml(xml_path)\n",
    "    # print(\"Ground truth\")\n",
    "    gt_name = f\"{i}_truth.png\"\n",
    "    plot_bboxes(img, gt_bboxes, gt_name, save_to_disk)\n",
    "\n",
    "\n",
    "def read_gt_xml(xml_path):\n",
    "    tree = ET.parse(open(xml_path, 'r'))\n",
    "\n",
    "    root=tree.getroot()\n",
    "\n",
    "    obj_list = []\n",
    "    objects = root.findall(\"object\")\n",
    "    for _object in objects:\n",
    "        name = _object.find(\"name\").text\n",
    "        bndbox = _object.find(\"bndbox\")\n",
    "        xmin = int(bndbox.find(\"xmin\").text)\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "        class_name = _object.find('name').text\n",
    "\n",
    "        obj_list.append({'class': voc_dataset.convert_labels[class_name], 'confidence': 1.0, \n",
    "                         'bbox': {'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax}})\n",
    "    return obj_list\n",
    "\n",
    "\n",
    "\n",
    "def plot_bboxes(img, bboxes, save_name, save_to_disk, color=(255, 69, 0)):\n",
    "    plot_cfg = {'bbox_color':color, 'bbox_thickness':2, \n",
    "                'font':cv2.FONT_HERSHEY_SIMPLEX, 'fontScale':0.5, 'fontColor':color, 'lineThickness':1}\n",
    "    img_bbox = img.copy()\n",
    "    img_bbox = cv2.cvtColor(img_bbox, cv2.COLOR_BGR2RGB)\n",
    "    for box in bboxes:\n",
    "        bbox = box['bbox']\n",
    "        cv2.rectangle(img_bbox, (bbox['x1'], bbox['y1']), (bbox['x2'], bbox['y2']), plot_cfg['bbox_color'], plot_cfg['bbox_thickness'])\n",
    "        cv2.putText(img_bbox, f\"{voc_dataset.label_type[box['class']]}, {str(box['confidence'])[:5]}\",  (bbox['x1'], bbox['y1'] - 5), plot_cfg['font'], \n",
    "                    plot_cfg['fontScale'], plot_cfg['fontColor'], plot_cfg['lineThickness'])\n",
    "    plt.imshow(img_bbox)\n",
    "    if save_to_disk:\n",
    "        plt.savefig(trainer.save_result_dir / save_name)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000027.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/158 [00:00<?, ?it/s]C:\\Users\\ton73\\AppData\\Local\\Temp\\ipykernel_2208\\4085269753.py:86: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = self.softmax(pred)\n",
      "100%|| 158/158 [00:07<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000063.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 167/167 [00:04<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000068.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 119/119 [00:02<00:00, 49.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000129.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 250/250 [00:06<00:00, 38.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000323.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 131/131 [00:02<00:00, 46.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000332.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 231/231 [00:05<00:00, 42.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000464.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 173/173 [00:03<00:00, 46.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000504.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 216/216 [00:04<00:00, 44.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000529.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 187/187 [00:03<00:00, 48.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000584.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 185/185 [00:03<00:00, 46.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000661.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 194/194 [00:04<00:00, 47.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000663.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 250/250 [00:06<00:00, 40.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000676.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 177/177 [00:03<00:00, 45.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000713.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 250/250 [00:06<00:00, 40.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000793.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 250/250 [00:06<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000799.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 231/231 [00:05<00:00, 39.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000830.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 158/158 [00:03<00:00, 44.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000836.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:04<00:00, 42.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_001027.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 250/250 [00:04<00:00, 52.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_001073.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 239/239 [00:05<00:00, 42.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_001154.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 209/209 [00:04<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_001225.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 193/193 [00:04<00:00, 44.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: data\\VOCtest\\VOCdevkit\\VOC2012\\JPEGImages\\2007_001284.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(voc_dataset\u001b[39m.\u001b[39mtest_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mJPEGImages\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m img_name)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mplotting:\u001b[39m\u001b[39m\"\u001b[39m, img_path)\n\u001b[1;32m----> 9\u001b[0m visualize(voc_dataset, model, img_path, i\u001b[39m=\u001b[39;49mi, save_to_disk\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(voc_dataset, model, img_path, i, save_to_disk)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize\u001b[39m(voc_dataset, model, img_path, i, save_to_disk\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_path)\n\u001b[1;32m----> 4\u001b[0m     pred_bboxes \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minference(img, apply_nms\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, nms_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m     pred_bboxes\u001b[39m=\u001b[39mnms(pred_bboxes, \u001b[39m0.2\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39m# plot predicted\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m# print(\"Predicted:\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 131\u001b[0m, in \u001b[0;36mRCNN.inference\u001b[1;34m(self, imgs, rgb, batch_size, apply_nms, nms_threshold)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference\u001b[39m(\u001b[39mself\u001b[39m, imgs, rgb\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, apply_nms\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, nms_threshold\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m):\n\u001b[0;32m    129\u001b[0m     \u001b[39m# when given single image\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(imgs) \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mndarray \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(imgs\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference_single(imgs, rgb, batch_size, apply_nms)\n\u001b[0;32m    133\u001b[0m     bboxes \u001b[39m=\u001b[39m []\n\u001b[0;32m    134\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m tqdm(imgs):\n",
      "Cell \u001b[1;32mIn[23], line 83\u001b[0m, in \u001b[0;36mRCNN.inference_single\u001b[1;34m(self, img, rgb, batch_size, apply_nms, nms_threshold)\u001b[0m\n\u001b[0;32m     80\u001b[0m patches \u001b[39m=\u001b[39m proposal_batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 83\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvnet(patches)\n\u001b[0;32m     84\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(features)\n\u001b[0;32m     85\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(features)\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\efficientnet_pytorch\\model.py:289\u001b[0m, in \u001b[0;36mEfficientNet.extract_features\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"use convolution layer to extract feature .\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39m    layer in the efficientnet model.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# Stem\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_swish(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bn0(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_stem(inputs)))\n\u001b[0;32m    291\u001b[0m \u001b[39m# Blocks\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39mfor\u001b[39;00m idx, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocks):\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\efficientnet_pytorch\\utils.py:80\u001b[0m, in \u001b[0;36mMemoryEfficientSwish.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mreturn\u001b[39;00m SwishImplementation\u001b[39m.\u001b[39;49mapply(x)\n",
      "File \u001b[1;32mc:\\Users\\ton73\\anaconda3\\envs\\rcnn\\lib\\site-packages\\efficientnet_pytorch\\utils.py:67\u001b[0m, in \u001b[0;36mSwishImplementation.forward\u001b[1;34m(ctx, i)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, i):\n\u001b[1;32m---> 67\u001b[0m     result \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49msigmoid(i)\n\u001b[0;32m     68\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(i)\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# plot and save\n",
    "trainer.save_result_dir = trainer.ckpt_dir / \"predictions\"\n",
    "trainer.save_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "plot_num = 200\n",
    "for i in range(plot_num):\n",
    "    img_name = sorted(os.listdir(voc_dataset.test_dir / \"JPEGImages\"))[i]\n",
    "    img_path = str(voc_dataset.test_dir / \"JPEGImages\" / img_name)\n",
    "    print(\"plotting:\", img_path)\n",
    "    visualize(voc_dataset, model, img_path, i=i, save_to_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efa59ba0514c88bea28add7efbea712f86beb100fe6589447027613f52b616e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
